{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Hedgehog Open Network Fabric is an open networking platform that brings the user experience enjoyed by so many in the public cloud to the private environments. Without vendor lock-in.</p> <p>Fabric is built around concept of VPCs (Virtual Private Clouds) similar to the public clouds and provides a multi-tenant API to define user intent on network isolation, connectivity and etc which gets automatically transformed into switches and software appliances configuration.</p> <p>You can read more about concepts and architecture in the documentation.</p> <p>You can find how to download and try Fabric on the self-hosted fully virtualized lab or on the hardware.</p>"},{"location":"architecture/fabric/","title":"Hedgehog Network Fabric","text":"<p>The Hedgehog Open Network Fabric is an open source network architecture that provides connectivity between virtual and physical workloads and provides a way to achieve network isolation between different groups of workloads using standard BGP EVPN and vxlan technology. The fabric provides a standard kubernetes interfaces to manage the elements in the physical network and provides a mechanism to configure virtual networks and define attachments to these virtual networks. The Hedgehog Fabric provides isolation between different groups of workloads by placing them in different virtual networks called VPC's. To achieve this we define different abstractions starting from the physical network where we define <code>Connection</code> which defines how a physical server on the network connects to a physical switch on the fabric.</p>"},{"location":"architecture/fabric/#underlay-network","title":"Underlay Network","text":"<p>The Hedgehog Fabric currently support two underlay network topologies.</p>"},{"location":"architecture/fabric/#collapsed-core","title":"Collapsed Core","text":"<p>A collapsed core topology is just a pair of switches connected in a mclag configuration with no other network elements. All workloads attach to these two switches.</p> <p></p> <p>The leaf's in this setup are configured to be in a mclag pair and servers can either be connected to both switches as a mclag port channel or as orphan ports connected to only one switch. both the leaves peer to external networks using BGP and act as gateway for workloads attached to them. The configuration of the underlay in the collapsed core is very simple and is ideal for very small deployments.</p>"},{"location":"architecture/fabric/#spine-leaf","title":"Spine - Leaf","text":"<p>A spine-leaf topology is a standard clos network with workloads attaching to leaf switches and spines providing connectivity between different leaves.</p> <p></p> <p>This kind of topology is useful for bigger deployments and provides all the advantages of a typical clos network. The underlay network is established using eBGP where each leaf has a separate ASN and peers will all spines in the network. We used RFC7938 as the reference for establishing the underlay network.</p>"},{"location":"architecture/fabric/#overlay-network","title":"Overlay Network","text":"<p>The overlay network runs on top the underlay network to create a virtual network. The overlay network isolates control and data plane traffic between different virtual networks and the underlay network. Visualization is achieved in the hedgehog fabric by encapsulating workload traffic over vxlan tunnels that are source and terminated on the leaf switches in the network. The fabric using BGP-EVPN/Vxlan to enable creation and management of virtual networks on top of the virtual. The fabric supports multiple virtual networks over the same underlay network to support multi-tenancy. Each virtual network in the hedgehog fabric is identified by a VPC. In the following sections we will dive a bit deeper into a high level overview of how are vpc's implemented in the hedgehog fabric and it's associated objects.</p>"},{"location":"architecture/fabric/#vpc","title":"VPC","text":"<p>We know what is a VPC and how to attach workloads to a specific VPC. Let us now take a look at how is this actually implemented on the network to provide the view of a private network.</p> <ul> <li>Each VPC is modeled as a vrf on each switch where there are VPC attachments defined for this vpc.    The Vrf is allocated its own VNI. The Vrf is local to each switch and the VNI is global for the entire fabric. By    mapping the vrf to a VNI and configuring an evpn instance in each vrf we establish a shared l3vni across the entire    fabric. All vrf participating in this vni can freely communicate with each other without a need for a policy. A Vlan    is allocated for each vrf which functions as a IRB Vlan for the vrf.</li> <li>The vrf created on each switch corresponding to a VPC configures a BGP instance with evpn to advertise its locally    attached subnets and import routes from its peered VPC's. The BGP instance in the tenant vrf's does not establish    neighbor relationships and is purely used to advertise locally attached routes into the VPC (all vrf's with the same    l3vni) across leafs in the network.</li> <li>A VPC can have multuple subnets. Each Subnet in the VPC is modeled as a Vlan on the switch. The vlan is only locally    significant and a given subnet might have different Vlan's on different leaves on the network. We assign a globally    significant vni for each subnet. This VNI is used to extend the subnet across different leaves in the network and    provides a view of single stretched l2 domain if the applications need it.</li> <li>The hedgehog fabric has a built-in DHCP server which will automatically assign IP addresses to each workload    depending on the VPC it belongs to. This is achieved by configuring a DHCP relay on each of the server facing vlans.    The DHCP server is accessible through the underlay network and is shared by all vpcs in the fabric. The inbuilt DHCP    server is capable of identifying the source VPC of the request and assigning IP addresses from a pool allocated to the    VPC at creation.</li> <li>A VPC by default cannot communicate to anyone outside the VPC and we need to define specific peering rules to allow    communication to external networks or to other VPCs.</li> </ul>"},{"location":"architecture/fabric/#vpc-peering","title":"VPC Peering","text":"<p>To enable communication between 2 different VPC's we need to configure a VPC peering policy. The hedgehog fabric supports two different peering modes.</p> <ul> <li>Local Peering - A local peering directly imports routers from the other VPC locally. This is achieved by a simple   import route from the peer VPC. In case there are no locally attached workloads to the peer VPC the fabric   automatically creates a stub vpc for peering and imports routes from it. This allows VPC's to peer with each other   without the need for dedicated peering leaf. If a local peering is done for a pair of VPC's which have locally   attached workloads the fabric automatically allocates a pair of ports on the switch to router traffic between these   vrf's using static routes. This is required because of limitations in  the underlying platform. The net result of   this is that the bandwidth between these 2 VPC's is limited by the bandwidth of the loopback interfaces allocated   on the switch.</li> <li>Remote Peering  - Remote peering is implemented using a dedicated peering switch/switches which is used as a   rendezvous point for the 2 VPC's in the fabric. The set of switches to be used for peering is determined by   configuration in the peering policy. When a remote peering policy is applied for a pair of VPC's the vrf's   corresponding to these VPC's on the peering switch advertise default routes into their specific vrf's identified   by the l3vni. All traffic that does not belong to the VPC's is forwarded to the peering switch and which has routes   to the other VPC's and gets forwarded from there. The bandwidth limitation that exists in the local peering solution   is solved here as the bandwidth between the two VPC's is determined by the fabric cross section bandwidth.</li> </ul>"},{"location":"architecture/overview/","title":"Overview","text":"<p>Under construction.</p>"},{"location":"concepts/overview/","title":"Concepts","text":""},{"location":"concepts/overview/#introduction","title":"Introduction","text":"<p>Hedgehog Open Network Fabric is build on top of Kubernetes and uses Kubernetes API to manage its resources. It means that all user-facing APIs are Kubernetes Custom Resources (CRDs) and so you can use standard Kubernetes tools to manage Fabric resources.</p> <p>Hedgehog Fabric consists of the following components:</p> <ul> <li>Fabricator - special tool that allows to install and configure Fabric as well as run virtual labs</li> <li>Control Node - one or more Kubernetes nodes in a single clusters running Fabric software<ul> <li>Das Boot - set of services providing switch boot and installation</li> <li>Fabric Controller - main control plane component that manages Fabric resources</li> </ul> </li> <li>Fabric Kubectl plugin (Fabric CLI) - plugin for kubectl that allows to manage Fabric resources in an easy way</li> <li>Fabric Agent - runs on every switch and manages switch configuration</li> </ul>"},{"location":"concepts/overview/#fabric-api","title":"Fabric API","text":"<p>All infrastructure is represented as a set of Fabric resource (Kubernetes CRDs) and named Wiring Diagram. It allows to define switches, servers, control nodes, external systems and connections between them in a single place and then use it to deploy and manage the whole infrastructure. On top of it Fabric provides a set of APIs to manage the VPCs and connections between them and between VPCs and External systems.</p>"},{"location":"concepts/overview/#wiring-diagram-api","title":"Wiring Diagram API","text":"<p>Wiring Diagram consists of the following resources:</p> <ul> <li>\"Devices\": describes any device in the Fabric<ul> <li>Switch: configuration of the switch, like port group speeds, port breakouts, switch IP/ASN, etc.</li> <li>Server: any physical server attached to the Fabric including control nodes</li> </ul> </li> <li>Connection: any logical connection for devices<ul> <li>usually it's a connection between two or more ports on two different devices</li> <li>incl. MCLAG Peer Link, Unbundled/MCLAG server connections, Fabric connection between spine and leaf etc.</li> </ul> </li> <li>VLANNamespace -&gt; non-overlapping VLAN ranges for attaching servers</li> <li>IPv4Namespace -&gt; non-overlapping IPv4 ranges for VPC subnets</li> </ul>"},{"location":"concepts/overview/#user-facing-api","title":"User-facing API","text":"<ul> <li>VPC API<ul> <li>VPC: Virtual Private Cloud, similar to the public cloud VPC it provides an isolated private network for the   resources with support for multiple subnets each with user-provided VLANs and on-demand DHCP</li> <li>VPCAttachment: represents a specific VPC subnet assignment to the Connection object which means exact server port to a VPC binding</li> <li>VPCPeering: enables VPC to VPC connectivity (could be Local where VPCs are used or Remote peering on the border/mixed leafs)</li> </ul> </li> <li>External API<ul> <li>External: definition of the \"external system\" to peer with (could be one or multiple devices such as edge/provider routers)</li> <li>ExternalAttachment: configuration for a specific switch (using Connection object) describing how it connects to an external system</li> <li>ExternalPeering: enables VPC to External connectivity by exposing specific VPC subnets to the external system and allowing inbound routes from it</li> </ul> </li> </ul>"},{"location":"concepts/overview/#fabricator","title":"Fabricator","text":"<p>Installer builder and VLAB.</p> <ul> <li>Installer builder based on a preset (currently vlab for virtual &amp; lab for physical)</li> <li>Main input - wiring diagram</li> <li>All input artifacts coming from OCI registry</li> <li>Always full airgap (everything running from private registry)</li> <li>Flatcar Linux for control node, generated ignition.json</li> <li>Automatic k3s installation and private registry setup</li> <li>All components and their dependencies running in K8s</li> <li>Integrated Virtual Lab (VLAB) management</li> <li>Future:</li> <li>In-cluster (control) Operator to manage all components</li> <li>Upgrades handling for everything starting control node OS</li> <li>Installation progress, status and retries</li> <li>Disaster recovery and backups</li> </ul>"},{"location":"concepts/overview/#das-boot","title":"Das Boot","text":"<p>Switch boot and installation.</p> <ul> <li>Seeder</li> <li>Actual switch provisioning</li> <li>ONIE on a switch discovers control node using LLDP</li> <li>It loads and runs our multi-stage installer<ul> <li>Network configuration &amp; identity setup</li> <li>Performs device registration</li> <li>Hedgehog identity partition gets created on the switch</li> <li>Downloads SONiC installer and runs it</li> <li>Downloads Agent and it's config and installs to the switch</li> </ul> </li> <li>Registration Controller</li> <li>Device identity and registration</li> <li>Actual SONiC installers</li> <li>Misc: rsyslog/ntp</li> </ul>"},{"location":"concepts/overview/#fabric","title":"Fabric","text":"<p>Control plane and switch agent.</p> <ul> <li>Currently Fabric is basically single controller manager running in K8s</li> <li>It includes controllers for different CRDs and needs</li> <li>For example, auto assigning VNIs to VPC or generating Agent config</li> <li>Additionally, it's running admission webhook for our CRD APIs</li> <li>Agent is watching for the corresponding Agent CRD in K8s API</li> <li>It applies the changes and saves new config locally</li> <li>It reports back some status and information back to API</li> <li>Can perform reinstall and reboot of SONiC</li> </ul>"},{"location":"contribute/docs/","title":"Documentation","text":""},{"location":"contribute/docs/#getting-started","title":"Getting started","text":"<p>This documentation is done using MkDocs with multiple plugins enabled. It's based on the Markdown, you can find basic syntax overview here.</p> <p>In order to contribute to the documentation, you'll need to have Git and Docker installed on your machine as well as any editor of your choice, preferably supporting Markdown preview. You can run the preview server using following command:</p> <pre><code>make serve\n</code></pre> <p>Now you can open continuously updated preview of your edits in browser at http://127.0.0.1:8000. Pages will be automatically updated while you're editing.</p> <p>Additionally you can run</p> <pre><code>make build\n</code></pre> <p>to make sure that your changes will be built correctly and doesn't break documentation.</p>"},{"location":"contribute/docs/#workflow","title":"Workflow","text":"<p>If you want to quick edit any page in the documentation, you can press the <code>Edit this page</code> icon at the top right of the page. It'll open the page in the GitHub editor. You can edit it and create a pull request with your changes.</p> <p>Please, never push to the <code>master</code> or <code>release/*</code> branches directly. Always create a pull request and wait for the review.</p> <p>Each pull request will be automatically built and preview will be deployed. You can find the link to the preview in the comments in pull request.</p>"},{"location":"contribute/docs/#repository","title":"Repository","text":"<p>Documentation is organized in per-release branches:</p> <ul> <li><code>master</code> - ongoing development, not released yet, referenced as <code>dev</code> version in the documentation</li> <li><code>release/alpha-1</code>/<code>release/alpha-2</code> - alpha releases, referenced as <code>alpha-1</code>/<code>alpha-2</code> versions in the documentation, if patches released for alpha-1, they'll be merged into <code>release/alpha-1</code> branch</li> <li><code>release/v1.0</code> - first stable release, referenced as <code>v1.0</code> version in the documentation, if patches (e.g. <code>v1.0.1</code>) released for v1.0, they'll be merged into <code>release/v1.0</code> branch</li> </ul> <p>Latest release branch is referenced as <code>latest</code> version in the documentation and will be used by default when you open the documentation.</p>"},{"location":"contribute/docs/#file-layout","title":"File layout","text":"<p>All documentation files are located in <code>docs</code> directory. Each file is a Markdown file with <code>.md</code> extension. You can create subdirectories to organize your files. Each directory can have a <code>.pages</code> file that overrides the default navigation order and titles.</p> <p>For example, top-level <code>.pages</code> in this repository looks like this:</p> <pre><code>nav:\n  - index.md\n  - getting-started\n  - concepts\n  - Wiring Diagram: wiring\n  - Install &amp; Upgrade: install-upgrade\n  - User Guide: user-guide\n  - Reference: reference\n  - Troubleshooting: troubleshooting\n  - ...\n  - release-notes\n  - contribute\n</code></pre> <p>Where you can add pages by file name like <code>index.md</code> and page title will be taked from the file (first line with <code>#</code>). Additionally, you can reference the whole directory to created nested section in navigation. You can also add custom titles by using <code>:</code> separator like <code>Wiring Diagram: wiring</code> where <code>Wiring Diagram</code> is a title and <code>wiring</code> is a file/directory name.</p> <p>More details in the MkDocs Pages plugin.</p>"},{"location":"contribute/docs/#abbreaviations","title":"Abbreaviations","text":"<p>You can find abbreviations in <code>includes/abbreviations.md</code> file. You can add various abbreviations there and all usages of the defined words in the documentation will get a highlight.</p> <p>For example, we have following in <code>includes/abbreviations.md</code>:</p> <pre><code>*[HHFab]: Hedgehog Fabricator - a tool for building Hedgehog Fabric\n</code></pre> <p>It'll highlight all usages of <code>HHFab</code> in the documentation and show a tooltip with the definition like this: HHFab.</p>"},{"location":"contribute/docs/#markdown-extensions","title":"Markdown extensions","text":"<p>We're using MkDocs Material theme with multiple extensions enabled. You can find detailed reference here, but here you can find some of the most useful ones.</p> <p>To view code for examples, please, check the source code of this page.</p>"},{"location":"contribute/docs/#text-formatting","title":"Text formatting","text":"<p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p> <p>Keyboard keys can be written like so:</p> <p>Ctrl+Alt+Del</p> <p>Amd inline icons/emojis can be added like this:</p> <pre><code>:fontawesome-regular-face-laugh-wink:\n:fontawesome-brands-twitter:{ .twitter }\n</code></pre> <p> </p>"},{"location":"contribute/docs/#admonitions","title":"Admonitions","text":"<p>Admonitions, also known as call-outs, are an excellent choice for including side content without significantly interrupting the document flow. Different types of admonitions are available, each with a unique icon and color. Details can be found here.</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"contribute/docs/#code-blocks","title":"Code blocks","text":"<p>Details can be found here.</p> <p>Simple code block with line nums and highlighted lines:</p> bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Code annotations:</p> <pre><code>theme:\n  features:\n    - content.code.annotate # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>"},{"location":"contribute/docs/#tabs","title":"Tabs","text":"<p>You can use Tabs to better organize content.</p> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"contribute/docs/#tables","title":"Tables","text":"Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"contribute/docs/#diagrams","title":"Diagrams","text":"<p>You can directly include Mermaid diagrams in your Markdown files. Details can be found here.</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"contribute/overview/","title":"Overview","text":"<p>Under construction.</p>"},{"location":"getting-started/download/","title":"Download","text":""},{"location":"getting-started/download/#getting-access","title":"Getting access","text":"<p>Prior to the General Availability, access to the full software is limited and requires Design Partner Agreement. Please submit a ticket with the request using Hedgehog Support Portal.</p> <p>After that you will be provided with the credentials to access the software on GitHub Package. In order to use it you need to login to the registry using the following command:</p> <pre><code>docker login ghcr.io\n</code></pre>"},{"location":"getting-started/download/#downloading-the-software","title":"Downloading the software","text":"<p>The main entry point for the software is the Hedgehog Fabricator CLI named <code>hhfab</code>. All software is published into the OCI registry GitHub Package including binaries, container images, helm charts and etc. The latest stable <code>hhfab</code> binary can be downloaded from the GitHub Package using the following command:</p> <pre><code>curl -fsSL https://i.hhdev.io/hhfab | bash\n</code></pre> <p>Or you can download a specific version using the following command:</p> <pre><code>curl -fsSL https://i.hhdev.io/hhfab | VERSION=alpha-X bash\n</code></pre> <p>The <code>VERSION</code> environment variable can be used to specify the version of the software to download. If it's not specified the latest release will be downloaded. You can pick specific release series (e.g. <code>alpha-2</code>) or specific release.</p> <p>It requires ORAS to be installed which is used to download the binary from the OCI registry and could be installed using following command:</p> <pre><code>curl -fsSL https://i.hhdev.io/oras | bash\n</code></pre> <p>Currently only Linux x86 is supported for running <code>hhfab</code>.</p>"},{"location":"getting-started/download/#next-steps","title":"Next steps","text":"<ul> <li>Concepts</li> <li>Virtual LAB</li> <li>Installation</li> <li>User guide</li> </ul>"},{"location":"install-upgrade/build-wiring/","title":"Build Wiring Diagram","text":"<p>Under construction.</p> <p>In the meantime, to have a look at the working wiring diagram for the Hedgehog Fabric, please run sample generator that produces VLAB-compatible wiring diagrams:</p> <pre><code>ubuntu@sl-dev:~$ hhfab wiring sample -h\nNAME:\n   hhfab wiring sample - sample wiring diagram (would work for vlab)\n\nUSAGE:\n   hhfab wiring sample [command options] [arguments...]\n\nOPTIONS:\n   --brief, -b                    brief output (only warn and error) (default: false)\n   --fabric-mode value, -m value  fabric mode (one of: collapsed-core, spine-leaf) (default: \"spine-leaf\")\n   --help, -h                     show help\n   --verbose, -v                  verbose output (includes debug) (default: false)\n\n   wiring generator options:\n\n   --chain-control-link         chain control links instead of all switches directly connected to control node if fabric mode is spine-leaf (default: false)\n   --control-links-count value  number of control links if chain-control-link is enabled (default: 0)\n   --fabric-links-count value   number of fabric links if fabric mode is spine-leaf (default: 0)\n   --mclag-leafs-count value    number of mclag leafs (should be even) (default: 0)\n   --mclag-peer-links value     number of mclag peer links for each mclag leaf (default: 0)\n   --mclag-session-links value  number of mclag session links for each mclag leaf (default: 0)\n   --orphan-leafs-count value   number of orphan leafs (default: 0)\n   --spines-count value         number of spines if fabric mode is spine-leaf (default: 0)\n   --vpc-loopbacks value        number of vpc loopbacks for each switch (default: 0)\n</code></pre>"},{"location":"install-upgrade/config/","title":"Fabric Configuration","text":"<ul> <li><code>--fabric-mode &lt;mode-name</code> (<code>collapsed-core</code> or <code>spine-leaf</code>) - Fabric mode to use, default is <code>spine-leaf</code>; in case     of <code>collapsed-core</code> mode, there will be no VXLAN configured and only 2 switches will be used</li> <li><code>--ntp-servers &lt;servers&gt;</code>- Comma-separated list of NTP servers to use, default is     <code>time.cloudflare.com,time1.google.com,time2.google.com,time3.google.com,time4.google.com</code>, it'll be used for both     control nodes and switches</li> <li><code>--dhcpd &lt;mode-name&gt;</code> (<code>isc</code> or <code>hedgehog</code>) - DHCP server to use, default is <code>isc</code>; <code>hedgehog</code> DHCP server enables     use of on-demand DHCP for multiple IPv4/VLAN namespaces and overlapping IP ranges as well as adds DHCP leases     into the Fabric API</li> </ul> <p>You can find more information about using <code>hhfab init</code> in the help message by running it with <code>--help</code> flag.</p>"},{"location":"install-upgrade/onie-update/","title":"ONIE Update/Upgrade","text":""},{"location":"install-upgrade/onie-update/#hedgehog-onie-honie-supported-systems","title":"Hedgehog ONIE (HONIE) Supported Systems","text":"<ul> <li> <p>DELL</p> </li> <li> <p>S5248F-ON</p> </li> <li> <p>S5232F-ON</p> </li> <li> <p>Edge-Core</p> </li> <li> <p>DCS501 (AS7726-32X)</p> </li> <li> <p>DCS203 (AS7326-56X)</p> </li> <li> <p>EPS203 (AS4630-54NPE)</p> </li> </ul>"},{"location":"install-upgrade/onie-update/#updating-onie","title":"Updating ONIE","text":"<ul> <li> <p>Via USB</p> </li> <li> <p>For this example we will be updating a DELL S5248 to Hedgehog ONIE (HONIE)</p> <ul> <li>Note: the USB port is on the back of the switch with the Management and Console</li> </ul> </li> <li> <p>Prepare the USB stick by burning the honie-usb.img to a 4G or larger USB drive</p> </li> <li> <p>Insert the USB drive into the switch</p> <ul> <li> <p>For example, to burn the file to disk X of an OSX machine</p> </li> <li> <p>sudo dd if=honie-usb.img of=/dev/rdiskX bs=1m</p> </li> </ul> </li> <li> <p>Boot into ONIE Installer</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> <p>*</p> </li> <li> <p>ONIE will install the ONIE update and reboot</p> <ul> <li>`ONIE: OS Install Mode ...</li> </ul> <p>Platform\u00a0 : x86_64-dellemc_s5200_c3538-r0</p> <p>Version \u00a0 : 3.40.1.1-7 &lt;- Non HONIE version</p> <p>Build Date: 2020-03-24T20:44-07:00</p> <p>Info: Mounting EFI System on /boot/efi ...</p> <p>Info: BIOS mode: UEFI</p> <p>Info: Making NOS install boot mode persistent.</p> <p>Info: Using eth0 MAC address: 3c:2c:30:66:f0:00</p> <p>Info: eth0:\u00a0 Checking link... up.</p> <p>Info: Trying DHCPv4 on interface: eth0</p> <p>Warning: Unable to configure interface using DHCPv4: eth0</p> <p>ONIE: Using link-local IPv4 addr: eth0: 169.254.95.249/16</p> <p>Starting: klogd... done.</p> <p>Starting: dropbear ssh daemon... done.</p> <p>Starting: telnetd... done.</p> <p>discover: installer mode detected.\u00a0 Running installer.</p> <p>Starting: discover... done.</p> <p>Please press Enter to activate this console. Info: eth0:\u00a0 Checking link... up.</p> <p>Info: Trying DHCPv4 on interface: eth0</p> <p>Warning: Unable to configure interface using DHCPv4: eth0</p> <p>ONIE: Using link-local IPv4 addr: eth0: 169.254.6.139/16</p> <p>ONIE: Starting ONIE Service Discovery</p> <p>Info: Attempting file://dev/sdb1/onie-installer-x86_64-dellemc_s5248f_c3538-r0 ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64-dellemc_s5248f_c3538-r0 ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64-dellemc_s5248f_c3538-r0.bin ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64-dellemc_s5248f_c3538.bin ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-dellemc_s5248f_c3538 ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-dellemc_s5248f_c3538.bin ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64-bcm ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64-bcm.bin ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64 ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer-x86_64.bin ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer ...</p> <p>Info: Attempting file://dev/mmcblk0p1/onie-installer.bin ...</p> <p>ONIE: Executing installer: file://dev/sdb1/onie-installer-x86_64-dellemc_s5248f_c3538-r0</p> <p>Verifying image checksum ... OK.</p> <p>Preparing image archive ... OK.</p> <p>ONIE: Version \u00a0 \u00a0 \u00a0 : 3.40.1.1-8 &lt;- HONIE Version</p> <p>ONIE: Architecture\u00a0 : x86_64</p> <p>ONIE: Machine \u00a0 \u00a0 \u00a0 : dellemc_s5200_c3538</p> <p>ONIE: Machine Rev \u00a0 : 0</p> <p>ONIE: Config Version: 1</p> <p>ONIE: Build Date\u00a0 \u00a0 : 2023-12-15T23:43+00:00</p> <p>Installing ONIE on: /dev/sda</p> <p>ONIE: NOS install successful: file://dev/sdb1/onie-installer-x86_64-dellemc_s5248f_c3538-r0</p> <p>ONIE: Rebooting...</p> <p>discover: installer mode detected.</p> <p>Stopping: discover...start-stop-daemon: warning: killing process 665: No such process</p> <p>Info: Unmounting kernel filesystems</p> <p>umount: can't unmount /: Invalid argument</p> <p>The system is going down NOW!</p> <p>Sent SIGTERM to all processes</p> <p>Sent SIGKILL to all processes</p> <p>Requesting system reboot`</p> </li> <li> <p>System is now ready for use</p> </li> </ul>"},{"location":"install-upgrade/overview/","title":"Install Fabric","text":"<p>Under construction.</p>"},{"location":"install-upgrade/overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have a machine with access to internet to use Fabricator and build installer</li> <li>Have a machine to install Fabric Control Node on with enough NICs to connect to at least one switch using Front Panel   ports and enough CPU and RAM (System Requirements) as well as IPMI access to it to install OS</li> <li>Have enough Supported Switches for your Fabric</li> </ul>"},{"location":"install-upgrade/overview/#main-steps","title":"Main steps","text":"<p>This chapter is dedicated to the Hedgehog Fabric installation on the bare-metal control node(s) and switches, their preparation and configuration.</p> <p>Please, get <code>hhfab</code> installed following instructions from the Download section.</p> <p>Main steps to install Fabric are:</p> <ol> <li>Install <code>hhfab</code> on the machines with access to internet<ol> <li>Prepare Wiring Diagram</li> <li>Select Fabric Configuration</li> <li>Build Control Node configuration and installer</li> </ol> </li> <li>Install Control Node<ol> <li>Install Flatcar Linux on the Control Node</li> <li>Upload and run Control Node installer on the Control Node</li> </ol> </li> <li>Prepare supported switches<ol> <li>Install Hedgehog ONiE (HONiE) on them</li> <li>Reboot them into ONiE Install Mode and they will be automatically provisioned</li> </ol> </li> </ol>"},{"location":"install-upgrade/overview/#build-control-node-configuration-and-installer","title":"Build Control Node configuration and installer","text":"<p>It's the only step that requires internet access to download artifacts and build installer.</p> <p>Once you've prepared Wiring Diagram, you can initialize Fabricator by running <code>hhfab init</code> command and passwing optional configuration into it as well as wiring diagram file(s) as flags. Additionally, there are a lot of customizations available as flags, e.g. to setup default credentials, keys and etc, please, refer to <code>hhfab init --help</code> for more.</p> <p>The <code>--dev</code> options allows to enable development mode which will enable default credentials and keys for the Control Node and switches:</p> <ul> <li>Default user with passwordless sudo for the control node and test servers is <code>core</code> with password <code>HHFab.Admin!</code>.</li> <li>Admin user with full access and passwordless sudo for the switches is <code>admin</code> with password <code>HHFab.Admin!</code>.</li> <li>Read-only, non-sudo user with access only to the switch CLI for the switches is <code>op</code> with password <code>HHFab.Op!</code>.</li> </ul> <p>Alternatively, you can pass your own credentials and keys using <code>--authorized-key</code> and <code>--control-password-hash</code> flags. Password hash can be generated using <code>openssl passwd -5</code> command. Further customizations are available in the config file that could be passed using <code>--config</code> flag.</p> <pre><code>hhfab init --preset lab --dev --wiring file1.yaml --wiring file2.yaml\nhhfab build\n</code></pre> <p>As a result, you will get the following files in the <code>.hhfab</code> directory or the one you've passed using <code>--basedir</code> flag:</p> <ul> <li><code>control-os/ignition.json</code> - ignition config for the control node to get OS installed</li> <li><code>control-install.tgz</code> - installer for the control node, it will be uploaded to the control node and run there</li> </ul>"},{"location":"install-upgrade/overview/#install-control-node","title":"Install Control Node","text":"<p>It's fully air-gapped installation and doesn't require internet access.</p> <p>Please, download latest stable Flatcar Container Linux ISO from the link and boot into it (using IPMI attaching media, USB stick or any other way).</p> <p>Once you've booted into the Flatcar installer, you need to download <code>ignition.json</code> built in the prvious step to it and run Flatcar installation:</p> <pre><code>sudo flatcar-install -d /dev/sda -i ignition.json\n</code></pre> <p>Where <code>/dev/sda</code> is a disk you want to install Control Node to and <code>ignition.json</code> is the <code>control-os/ignition.json</code> file from previous step downloaded to the Flatcar installer.</p> <p>Once the installation is finished, reboot the machine and wait for it to boot into the installed Flatcar Linux.</p> <p>At that point, you should get into the installed Flatcar Linux using the dev or provided credentials with user <code>core</code> and you can now install Hedgehog Open Network Fabric on it. Download <code>control-install.tgz</code> to the just installed Control Node (e.g. by using scp) and run it.</p> <pre><code>tar xzf control-install.tgz &amp;&amp; cd control-install &amp;&amp; sudo ./hhfab-recipe run\n</code></pre> <p>It'll output log of installing the Fabric (including Kubernetes cluster, OCI registry misc components and etc), you should see following output in the end:</p> <pre><code>...\n01:34:45 INF Running name=reloader-image op=\"push fabricator/reloader:v1.0.40\"\n01:34:47 INF Running name=reloader-chart op=\"push fabricator/charts/reloader:1.0.40\"\n01:34:47 INF Running name=reloader-install op=\"file /var/lib/rancher/k3s/server/manifests/hh-reloader-install.yaml\"\n01:34:47 INF Running name=reloader-wait op=\"wait deployment/reloader-reloader\"\ndeployment.apps/reloader-reloader condition met\n01:35:15 INF Done took=3m39.586394608s\n</code></pre> <p>At that point, you can start interacting with the Fabric using <code>kubectl</code>, <code>kubectl fabric</code> and <code>k9s</code> preinstalled as part of the Control Node installer.</p> <p>You can now get HONiE installed on your switches and reboot them into ONiE Install Mode and they will be automatically provisioned from the Control Node.</p>"},{"location":"install-upgrade/requirements/","title":"System Requirements","text":"<ul> <li>Fast SSDs for system/root and K8s &amp; container runtime folders are required for stable work</li> <li>SSDs are mandatory for Control Nodes</li> <li>Minimal (non-HA) setup is a single Control Node</li> <li>(Future) Full (HA) setup is at least 3 Control Nodes</li> <li>(Future) Extra nodes could be used for things like Logging, Monitoring, Alerting stack and etc.</li> </ul>"},{"location":"install-upgrade/requirements/#non-ha-minimal-setup-1-control-node","title":"Non-HA (minimal) setup - 1 Control Node","text":"<ul> <li>Control Node runs non-HA K8s Control Plane installation with non-HA Hedgehog Fabric Control Plane on top of it</li> <li>Not recommended for more then 10 devices participating in the Hedgehog Fabric or production deployments</li> </ul> Minimal Recommended CPU 4 8 RAM 12 GB 16 GB Disk 100 GB 250 GB"},{"location":"install-upgrade/requirements/#future-ha-setup-3-control-nodes-per-node","title":"(Future) HA setup - 3+ Control Nodes (per node)","text":"<ul> <li>Each Control Node runs part of the HA K8s Control Plane installation with Hedgehog Fabric Control Plane on top of it in   HA mode as well</li> <li>Recommended for all cases where more then 10 devices participating in the Hedgehog Fabric</li> </ul> Minimal Recommended CPU 4 8 RAM 12 GB 16 GB Disk 100 GB 250 GB"},{"location":"install-upgrade/requirements/#device-participating-in-the-hedgehog-fabric-eg-switch","title":"Device participating in the Hedgehog Fabric (e.g. switch)","text":"<ul> <li>(Future) Each participating device is part of the K8s cluster, so, it run K8s kubelet</li> <li>Additionally it run Hedgehog Fabric Agent that controls devices configuration</li> </ul> Minimal Recommended CPU 1 2 RAM 1 GB 1.5 GB Disk 5 GB 10 GB"},{"location":"install-upgrade/supported-devices/","title":"Supported Devices","text":""},{"location":"install-upgrade/supported-devices/#spine","title":"Spine","text":"<ul> <li>DELL: S5232F-ON</li> <li>EDGE-CORE: DCS204 (AS7726-54x)</li> </ul>"},{"location":"install-upgrade/supported-devices/#leaf","title":"Leaf","text":"<ul> <li>DELL: S5232F-ON, S5248F-ON</li> <li>EDGE-CORE: DCS204 (AS7726-32x), DCS203 (AS7326-56x), EPS203 (AS4630-54NPE)</li> </ul>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#packages","title":"Packages","text":"<ul> <li>agent.githedgehog.com/v1alpha2</li> <li>dhcp.githedgehog.com/v1alpha2</li> <li>vpc.githedgehog.com/v1alpha2</li> <li>wiring.githedgehog.com/v1alpha2</li> </ul>"},{"location":"reference/api/#agentgithedgehogcomv1alpha2","title":"agent.githedgehog.com/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the agent v1alpha2 API group. This is the internal API group for the switch and control node agents. Not intended to be modified by the user.</p>"},{"location":"reference/api/#resource-types","title":"Resource Types","text":"<ul> <li>Agent</li> </ul>"},{"location":"reference/api/#agent","title":"Agent","text":"<p>Agent is an internal API object used by the controller to pass all relevant information to the agent running on a specific switch in order to fully configure it and manage its lifecycle. It is not intended to be used directly by users. Spec of the object isn't user-editable, it is managed by the controller. Status of the object is updated by the agent and is used by the controller to track the state of the agent and the switch it is running on. Name of the Agent object is the same as the name of the switch it is running on and it's created in the same namespace as the Switch object.</p> Field Description <code>apiVersion</code> string <code>agent.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>Agent</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> AgentSpec Spec is the desired state of the Agent <code>status</code> AgentStatus Status is the observed state of the Agent"},{"location":"reference/api/#agentstatus","title":"AgentStatus","text":"<p>AgentStatus defines the observed state of the agent running on a specific switch and includes information about the switch itself as well as the state of the agent and applied configuration.</p> <p>Appears in: - Agent</p> Field Description <code>version</code> string Current running agent version <code>installID</code> string ID of the agent installation, used to track NOS re-installs <code>runID</code> string ID of the agent run, used to track NOS reboots <code>lastHeartbeat</code> Time Time of the last heartbeat from the agent <code>lastAttemptTime</code> Time Time of the last attempt to apply configuration <code>lastAttemptGen</code> integer Generation of the last attempt to apply configuration <code>lastAppliedTime</code> Time Time of the last successful configuration application <code>lastAppliedGen</code> integer Generation of the last successful configuration application <code>nosInfo</code> NOSInfo Information about the switch and NOS <code>statusUpdates</code> ApplyStatusUpdate array Status updates from the agent <code>conditions</code> Condition array Conditions of the agent, includes readiness marker for use with kubectl wait"},{"location":"reference/api/#nosinfo","title":"NOSInfo","text":"<p>NOSInfo contains information about the switch and NOS received from the switch itself by the agent</p> <p>Appears in: - AgentStatus</p> Field Description <code>asicVersion</code> string ASIC name, such as \"broadcom\" or \"vs\" <code>buildCommit</code> string NOS build commit <code>buildDate</code> string NOS build date <code>builtBy</code> string NOS build user <code>configDbVersion</code> string NOS config DB version, such as \"version_4_2_1\" <code>distributionVersion</code> string Distribution version, such as \"Debian 10.13\" <code>hardwareVersion</code> string Hardware version, such as \"X01\" <code>hwskuVersion</code> string Hwsku version, such as \"DellEMC-S5248f-P-25G-DPB\" <code>kernelVersion</code> string Kernel version, such as \"5.10.0-21-amd64\" <code>mfgName</code> string Manufacturer name, such as \"Dell EMC\" <code>platformName</code> string Platform name, such as \"x86_64-dellemc_s5248f_c3538-r0\" <code>productDescription</code> string NOS product description, such as \"Enterprise SONiC Distribution by Broadcom - Enterprise Base package\" <code>productVersion</code> string NOS product version, empty for Broadcom SONiC <code>serialNumber</code> string Switch serial number <code>softwareVersion</code> string NOS software version, such as \"4.2.0-Enterprise_Base\" <code>upTime</code> string Switch uptime, such as \"21:21:27 up 1 day, 23:26, 0 users, load average: 1.92, 1.99, 2.00 \""},{"location":"reference/api/#dhcpgithedgehogcomv1alpha2","title":"dhcp.githedgehog.com/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the dhcp v1alpha2 API group. It is the primarely internal API group for the intended Hedgehog DHCP server configuration and storing leases as well as making them available to the end user through API. Not intended to be modified by the user.</p>"},{"location":"reference/api/#resource-types_1","title":"Resource Types","text":"<ul> <li>DHCPSubnet</li> </ul>"},{"location":"reference/api/#dhcpallocated","title":"DHCPAllocated","text":"<p>DHCPAllocated is a single allocated IP with expiry time and hostname from DHCP requests, it's effectively a DHCP lease</p> <p>Appears in: - DHCPSubnetStatus</p> Field Description <code>ip</code> string Allocated IP address <code>expiry</code> Time Expiry time of the lease <code>hostname</code> string Hostname from DHCP request"},{"location":"reference/api/#dhcpsubnet","title":"DHCPSubnet","text":"<p>DHCPSubnet is the configuration (spec) for the Hedgehog DHCP server and storage for the leases (status). It's primarely internal API group, but it makes allocated IPs / leases information available to the end user through API. Not intended to be modified by the user.</p> Field Description <code>apiVersion</code> string <code>dhcp.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>DHCPSubnet</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> DHCPSubnetSpec Spec is the desired state of the DHCPSubnet <code>status</code> DHCPSubnetStatus Status is the observed state of the DHCPSubnet"},{"location":"reference/api/#dhcpsubnetspec","title":"DHCPSubnetSpec","text":"<p>DHCPSubnetSpec defines the desired state of DHCPSubnet</p> <p>Appears in: - DHCPSubnet</p> Field Description <code>subnet</code> string Full VPC subnet name (including VPC name), such as \"vpc-0/default\" <code>cidrBlock</code> string CIDR block to use for VPC subnet, such as \"10.10.10.0/24\" <code>gateway</code> string Gateway, such as 10.10.10.1 <code>startIP</code> string Start IP from the CIDRBlock to allocate IPs, such as 10.10.10.10 <code>endIP</code> string End IP from the CIDRBlock to allocate IPs, such as 10.10.10.99 <code>vrf</code> string VRF name to identify specific VPC (will be added to DHCP packets by DHCP relay in suboption 151), such as \"VrfVvpc-1\" as it's named on switch <code>circuitID</code> string VLAN ID to identify specific subnet withing the VPC, such as \"Vlan1000\" as it's named on switch"},{"location":"reference/api/#dhcpsubnetstatus","title":"DHCPSubnetStatus","text":"<p>DHCPSubnetStatus defines the observed state of DHCPSubnet</p> <p>Appears in: - DHCPSubnet</p> Field Description <code>allocated</code> object (keys:string, values:DHCPAllocated) Allocated is a map of allocated IPs with expiry time and hostname from DHCP requests"},{"location":"reference/api/#vpcgithedgehogcomv1alpha2","title":"vpc.githedgehog.com/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the vpc v1alpha2 API group. It is public API group for the VPCs and Externals APIs. Intended to be used by the user.</p>"},{"location":"reference/api/#resource-types_2","title":"Resource Types","text":"<ul> <li>External</li> <li>ExternalAttachment</li> <li>ExternalPeering</li> <li>IPv4Namespace</li> <li>VPC</li> <li>VPCAttachment</li> <li>VPCPeering</li> </ul>"},{"location":"reference/api/#external","title":"External","text":"<p>External object represents an external system connected to the Fabric and available to the specific IPv4Namespace. Users can do external peering with the external system by specifying the name of the External Object without need to worry about the details of how external system is attached to the Fabric.</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>External</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ExternalSpec Spec is the desired state of the External <code>status</code> ExternalStatus Status is the observed state of the External"},{"location":"reference/api/#externalattachment","title":"ExternalAttachment","text":"<p>ExternalAttachment is a definition of how specific switch is connected with external system (External object). Effectively it represents BGP peering between the switch and external system including all needed configuration.</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>ExternalAttachment</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ExternalAttachmentSpec Spec is the desired state of the ExternalAttachment <code>status</code> ExternalAttachmentStatus Status is the observed state of the ExternalAttachment"},{"location":"reference/api/#externalattachmentneighbor","title":"ExternalAttachmentNeighbor","text":"<p>ExternalAttachmentNeighbor defines the BGP neighbor configuration for the external attachment</p> <p>Appears in: - ExternalAttachmentSpec</p> Field Description <code>asn</code> integer ASN is the ASN of the BGP neighbor <code>ip</code> string IP is the IP address of the BGP neighbor to peer with"},{"location":"reference/api/#externalattachmentspec","title":"ExternalAttachmentSpec","text":"<p>ExternalAttachmentSpec defines the desired state of ExternalAttachment</p> <p>Appears in: - AgentSpec - ExternalAttachment</p> Field Description <code>external</code> string External is the name of the External object this attachment belongs to <code>connection</code> string Connection is the name of the Connection object this attachment belongs to (essentialy the name of the switch/port) <code>switch</code> ExternalAttachmentSwitch Switch is the switch port configuration for the external attachment <code>neighbor</code> ExternalAttachmentNeighbor Neighbor is the BGP neighbor configuration for the external attachment"},{"location":"reference/api/#externalattachmentstatus","title":"ExternalAttachmentStatus","text":"<p>ExternalAttachmentStatus defines the observed state of ExternalAttachment</p> <p>Appears in: - ExternalAttachment</p>"},{"location":"reference/api/#externalattachmentswitch","title":"ExternalAttachmentSwitch","text":"<p>ExternalAttachmentSwitch defines the switch port configuration for the external attachment</p> <p>Appears in: - ExternalAttachmentSpec</p> Field Description <code>vlan</code> integer VLAN is the VLAN ID used for the subinterface on a switch port specified in the connection <code>ip</code> string IP is the IP address of the subinterface on a switch port specified in the connection"},{"location":"reference/api/#externalpeering","title":"ExternalPeering","text":"<p>ExternalPeering is the Schema for the externalpeerings API</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>ExternalPeering</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ExternalPeeringSpec Spec is the desired state of the ExternalPeering <code>status</code> ExternalPeeringStatus Status is the observed state of the ExternalPeering"},{"location":"reference/api/#externalpeeringspec","title":"ExternalPeeringSpec","text":"<p>ExternalPeeringSpec defines the desired state of ExternalPeering</p> <p>Appears in: - AgentSpec - ExternalPeering</p> Field Description <code>permit</code> ExternalPeeringSpecPermit Permit defines the peering policy - which VPC and External to peer with and which subnets/prefixes to permit"},{"location":"reference/api/#externalpeeringspecexternal","title":"ExternalPeeringSpecExternal","text":"<p>ExternalPeeringSpecExternal defines the External-side of the configuration to peer with</p> <p>Appears in: - ExternalPeeringSpecPermit</p> Field Description <code>name</code> string Name is the name of the External to peer with <code>prefixes</code> ExternalPeeringSpecPrefix array Prefixes is the list of prefixes to permit from the External to the VPC"},{"location":"reference/api/#externalpeeringspecpermit","title":"ExternalPeeringSpecPermit","text":"<p>ExternalPeeringSpecPermit defines the peering policy - which VPC and External to peer with and which subnets/prefixes to permit</p> <p>Appears in: - ExternalPeeringSpec</p> Field Description <code>vpc</code> ExternalPeeringSpecVPC VPC is the VPC-side of the configuration to peer with <code>external</code> ExternalPeeringSpecExternal External is the External-side of the configuration to peer with"},{"location":"reference/api/#externalpeeringspecprefix","title":"ExternalPeeringSpecPrefix","text":"<p>ExternalPeeringSpecPrefix defines the prefix to permit from the External to the VPC</p> <p>Appears in: - ExternalPeeringSpecExternal</p> Field Description <code>prefix</code> string Prefix is the subnet to permit from the External to the VPC, e.g. 0.0.0.0/0 for default route <code>ge</code> integer Ge is the minimum prefix length to permit from the External to the VPC, e.g. 24 for /24 <code>le</code> integer Le is the maximum prefix length to permit from the External to the VPC, e.g. 32 for /32"},{"location":"reference/api/#externalpeeringspecvpc","title":"ExternalPeeringSpecVPC","text":"<p>ExternalPeeringSpecVPC defines the VPC-side of the configuration to peer with</p> <p>Appears in: - ExternalPeeringSpecPermit</p> Field Description <code>name</code> string Name is the name of the VPC to peer with <code>subnets</code> string array Subnets is the list of subnets to advertise from VPC to the External"},{"location":"reference/api/#externalpeeringstatus","title":"ExternalPeeringStatus","text":"<p>ExternalPeeringStatus defines the observed state of ExternalPeering</p> <p>Appears in: - ExternalPeering</p>"},{"location":"reference/api/#externalspec","title":"ExternalSpec","text":"<p>ExternalSpec describes IPv4 namespace External belongs to and inbound/outbound communities which are used to filter routes from/to the external system.</p> <p>Appears in: - AgentSpec - External</p> Field Description <code>ipv4Namespace</code> string IPv4Namespace is the name of the IPv4Namespace this External belongs to <code>inboundCommunity</code> string InboundCommunity is the name of the inbound community to filter routes from the external system <code>outboundCommunity</code> string OutboundCommunity is the name of the outbound community that all outbound routes will be stamped with"},{"location":"reference/api/#externalstatus","title":"ExternalStatus","text":"<p>ExternalStatus defines the observed state of External</p> <p>Appears in: - External</p>"},{"location":"reference/api/#ipv4namespace","title":"IPv4Namespace","text":"<p>IPv4Namespace represents a namespace for VPC subnets allocation. All VPC subnets withing a single IPv4Namespace are non-overlapping. Users can create multiple IPv4Namespaces to allocate same VPC subnets.</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>IPv4Namespace</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> IPv4NamespaceSpec Spec is the desired state of the IPv4Namespace <code>status</code> IPv4NamespaceStatus Status is the observed state of the IPv4Namespace"},{"location":"reference/api/#ipv4namespacespec","title":"IPv4NamespaceSpec","text":"<p>IPv4NamespaceSpec defines the desired state of IPv4Namespace</p> <p>Appears in: - AgentSpec - IPv4Namespace</p> Field Description <code>subnets</code> string array Subnets is the list of subnets to allocate VPC subnets from, couldn't overlap between each other and with Fabric reserved subnets"},{"location":"reference/api/#ipv4namespacestatus","title":"IPv4NamespaceStatus","text":"<p>IPv4NamespaceStatus defines the observed state of IPv4Namespace</p> <p>Appears in: - IPv4Namespace</p>"},{"location":"reference/api/#vpc","title":"VPC","text":"<p>VPC is Virtual Private Cloud, similar to the public cloud VPC it provides an isolated private network for the resources with support for multiple subnets each with user-provided VLANs and on-demand DHCP.</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>VPC</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> VPCSpec Spec is the desired state of the VPC <code>status</code> VPCStatus Status is the observed state of the VPC"},{"location":"reference/api/#vpcattachment","title":"VPCAttachment","text":"<p>VPCAttachment is the Schema for the vpcattachments API</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>VPCAttachment</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> VPCAttachmentSpec Spec is the desired state of the VPCAttachment <code>status</code> VPCAttachmentStatus Status is the observed state of the VPCAttachment"},{"location":"reference/api/#vpcattachmentspec","title":"VPCAttachmentSpec","text":"<p>VPCAttachmentSpec defines the desired state of VPCAttachment</p> <p>Appears in: - AgentSpec - VPCAttachment</p> Field Description <code>subnet</code> string Subnet is the full name of the VPC subnet to attach to, such as \"vpc-1/default\" <code>connection</code> string Connection is the name of the connection to attach to the VPC"},{"location":"reference/api/#vpcattachmentstatus","title":"VPCAttachmentStatus","text":"<p>VPCAttachmentStatus defines the observed state of VPCAttachment</p> <p>Appears in: - VPCAttachment</p>"},{"location":"reference/api/#vpcdhcp","title":"VPCDHCP","text":"<p>VPCDHCP defines the on-demand DHCP configuration for the subnet</p> <p>Appears in: - VPCSubnet</p> Field Description <code>relay</code> string Relay is the DHCP relay IP address, if specified, DHCP server will be disabled <code>enable</code> boolean Enable enables DHCP server for the subnet <code>range</code> VPCDHCPRange Range is the DHCP range for the subnet if DHCP server is enabled"},{"location":"reference/api/#vpcdhcprange","title":"VPCDHCPRange","text":"<p>Underlying type: struct{Start string \"json:\\\"start,omitempty\\\"\"; End string \"json:\\\"end,omitempty\\\"\"}</p> <p>VPCDHCPRange defines the DHCP range for the subnet if DHCP server is enabled</p> <p>Appears in: - VPCDHCP</p>"},{"location":"reference/api/#vpcpeer","title":"VPCPeer","text":"<p>Appears in: - VPCPeeringSpec</p> Field Description <code>subnets</code> string array Subnets is the list of subnets to advertise from current VPC to the peer VPC"},{"location":"reference/api/#vpcpeering","title":"VPCPeering","text":"<p>VPCPeering represents a peering between two VPCs with corresponding filtering rules. Minimal example of the VPC peering showing vpc-1 to vpc-2 peering with all subnets allowed:   spec: permit: - vpc-1: {} vpc-2: {}</p> Field Description <code>apiVersion</code> string <code>vpc.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>VPCPeering</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> VPCPeeringSpec Spec is the desired state of the VPCPeering <code>status</code> VPCPeeringStatus Status is the observed state of the VPCPeering"},{"location":"reference/api/#vpcpeeringspec","title":"VPCPeeringSpec","text":"<p>VPCPeeringSpec defines the desired state of VPCPeering</p> <p>Appears in: - AgentSpec - VPCPeering</p> Field Description <code>remote</code> string <code>permit</code> map[string]VPCPeer array Permit defines a list of the peering policies - which VPC subnets will have access to the peer VPC subnets."},{"location":"reference/api/#vpcpeeringstatus","title":"VPCPeeringStatus","text":"<p>VPCPeeringStatus defines the observed state of VPCPeering</p> <p>Appears in: - VPCPeering</p>"},{"location":"reference/api/#vpcspec","title":"VPCSpec","text":"<p>VPCSpec defines the desired state of VPC. At least one subnet is required.</p> <p>Appears in: - AgentSpec - VPC</p> Field Description <code>subnets</code> object (keys:string, values:VPCSubnet) Subnets is the list of VPC subnets to configure <code>ipv4Namespace</code> string IPv4Namespace is the name of the IPv4Namespace this VPC belongs to (if not specified, \"default\" is used) <code>vlanNamespace</code> string VLANNamespace is the name of the VLANNamespace this VPC belongs to (if not specified, \"default\" is used) <code>defaultIsolated</code> boolean DefaultIsolated sets default bahivour for isolated mode for the subnets (disabled by default) <code>defaultRestricted</code> boolean DefaultRestricted sets default bahivour for restricted mode for the subnets (disabled by default) <code>permit</code> string array array Permit defines a list of the access policies between the subnets within the VPC - each policy is a list of subnets that have access to each other. It's applied on top of the subnet isolation flag and if subnet isn't isolated it's not required to have it in a permit list while if vpc is marked as isolated it's required to have it in a permit list to have access to other subnets."},{"location":"reference/api/#vpcstatus","title":"VPCStatus","text":"<p>VPCStatus defines the observed state of VPC</p> <p>Appears in: - VPC</p>"},{"location":"reference/api/#vpcsubnet","title":"VPCSubnet","text":"<p>VPCSubnet defines the VPC subnet configuration</p> <p>Appears in: - VPCSpec</p> Field Description <code>subnet</code> string Subnet is the subnet CIDR block, such as \"10.0.0.0/24\", should belong to the IPv4Namespace and be unique within the namespace <code>dhcp</code> VPCDHCP DHCP is the on-demand DHCP configuration for the subnet <code>vlan</code> string VLAN is the VLAN ID for the subnet, should belong to the VLANNamespace and be unique within the namespace <code>isolated</code> boolean Isolated is the flag to enable isolated mode for the subnet which means no access to and from the other subnets within the VPC <code>restricted</code> boolean Restricted is the flag to enable restricted mode for the subnet which means no access between hosts within the subnet itself"},{"location":"reference/api/#wiringgithedgehogcomv1alpha2","title":"wiring.githedgehog.com/v1alpha2","text":"<p>Package v1alpha2 contains API Schema definitions for the wiring v1alpha2 API group. It is public API group mainly for the underlay definition including Switches, Server, wiring between them and etc. Intended to be used by the user.</p>"},{"location":"reference/api/#resource-types_3","title":"Resource Types","text":"<ul> <li>Connection</li> <li>Rack</li> <li>Server</li> <li>Switch</li> <li>SwitchGroup</li> <li>VLANNamespace</li> </ul>"},{"location":"reference/api/#baseportname","title":"BasePortName","text":"<p>BasePortName defines the full name of the switch port</p> <p>Appears in: - ConnExternalLink - ConnFabricLinkSwitch - ConnMgmtLinkServer - ConnMgmtLinkSwitch - ConnStaticExternalLinkSwitch - ServerToSwitchLink - SwitchToSwitchLink</p> Field Description <code>port</code> string Port defines the full name of the switch port in the format of \"device/port\", such as \"spine-1/Ethernet1\". SONiC port name is used as a port name and switch name should be same as the name of the Switch object."},{"location":"reference/api/#connbundled","title":"ConnBundled","text":"<p>ConnBundled defines the bundled connection (port channel, single server to a single switch with multiple links)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>links</code> ServerToSwitchLink array Links is the list of server-to-switch links <code>mtu</code> integer MTU is the MTU to be configured on the switch port or port channel"},{"location":"reference/api/#conneslag","title":"ConnESLAG","text":"<p>ConnESLAG defines the ESLAG connection (port channel, single server to 2-4 switches with multiple links)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>links</code> ServerToSwitchLink array Links is the list of server-to-switch links <code>mtu</code> integer MTU is the MTU to be configured on the switch port or port channel"},{"location":"reference/api/#connexternal","title":"ConnExternal","text":"<p>ConnExternal defines the external connection (single switch to a single external device with a single link)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>link</code> ConnExternalLink Link is the external connection link"},{"location":"reference/api/#connexternallink","title":"ConnExternalLink","text":"<p>ConnExternalLink defines the external connection link</p> <p>Appears in: - ConnExternal</p> Field Description <code>switch</code> BasePortName"},{"location":"reference/api/#connfabric","title":"ConnFabric","text":"<p>ConnFabric defines the fabric connection (single spine to a single leaf with at least one link)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>links</code> FabricLink array Links is the list of spine-to-leaf links"},{"location":"reference/api/#connfabriclinkswitch","title":"ConnFabricLinkSwitch","text":"<p>ConnFabricLinkSwitch defines the switch side of the fabric link</p> <p>Appears in: - FabricLink</p> Field Description <code>port</code> string Port defines the full name of the switch port in the format of \"device/port\", such as \"spine-1/Ethernet1\". SONiC port name is used as a port name and switch name should be same as the name of the Switch object. <code>ip</code> string IP is the IP address of the switch side of the fabric link (switch port configuration)"},{"location":"reference/api/#connmclag","title":"ConnMCLAG","text":"<p>ConnMCLAG defines the MCLAG connection (port channel, single server to pair of switches with multiple links)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>links</code> ServerToSwitchLink array Links is the list of server-to-switch links <code>mtu</code> integer MTU is the MTU to be configured on the switch port or port channel"},{"location":"reference/api/#connmclagdomain","title":"ConnMCLAGDomain","text":"<p>ConnMCLAGDomain defines the MCLAG domain connection which makes two switches into a single logical switch or redundancy group and allows to use MCLAG connections to connect servers in a multi-homed way.</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>peerLinks</code> SwitchToSwitchLink array PeerLinks is the list of peer links between the switches, used to pass server traffic between switch <code>sessionLinks</code> SwitchToSwitchLink array SessionLinks is the list of session links between the switches, used only to pass MCLAG control plane and BGP traffic between switches"},{"location":"reference/api/#connmgmt","title":"ConnMgmt","text":"<p>ConnMgmt defines the management connection (single control node/server to a single switch with a single link)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>link</code> ConnMgmtLink"},{"location":"reference/api/#connmgmtlink","title":"ConnMgmtLink","text":"<p>ConnMgmtLink defines the management connection link</p> <p>Appears in: - ConnMgmt</p> Field Description <code>server</code> ConnMgmtLinkServer Server is the server side of the management link <code>switch</code> ConnMgmtLinkSwitch Switch is the switch side of the management link"},{"location":"reference/api/#connmgmtlinkserver","title":"ConnMgmtLinkServer","text":"<p>ConnMgmtLinkServer defines the server side of the management link</p> <p>Appears in: - ConnMgmtLink</p> Field Description <code>port</code> string Port defines the full name of the switch port in the format of \"device/port\", such as \"spine-1/Ethernet1\". SONiC port name is used as a port name and switch name should be same as the name of the Switch object. <code>ip</code> string IP is the IP address of the server side of the management link (control node port configuration) <code>mac</code> string MAC is an optional MAC address of the control node port for the management link, if specified will be used to create a \"virtual\" link with the connection names on the control node"},{"location":"reference/api/#connmgmtlinkswitch","title":"ConnMgmtLinkSwitch","text":"<p>ConnMgmtLinkSwitch defines the switch side of the management link</p> <p>Appears in: - ConnMgmtLink</p> Field Description <code>port</code> string Port defines the full name of the switch port in the format of \"device/port\", such as \"spine-1/Ethernet1\". SONiC port name is used as a port name and switch name should be same as the name of the Switch object. <code>ip</code> string IP is the IP address of the switch side of the management link (switch port configuration) <code>oniePortName</code> string ONIEPortName is an optional ONIE port name of the switch side of the management link that's only used by the IPv6 Link Local discovery"},{"location":"reference/api/#connstaticexternal","title":"ConnStaticExternal","text":"<p>ConnStaticExternal defines the static external connection (single switch to a single external device with a single link)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>link</code> ConnStaticExternalLink Link is the static external connection link"},{"location":"reference/api/#connstaticexternallink","title":"ConnStaticExternalLink","text":"<p>ConnStaticExternalLink defines the static external connection link</p> <p>Appears in: - ConnStaticExternal</p> Field Description <code>switch</code> ConnStaticExternalLinkSwitch Switch is the switch side of the static external connection link"},{"location":"reference/api/#connstaticexternallinkswitch","title":"ConnStaticExternalLinkSwitch","text":"<p>ConnStaticExternalLinkSwitch defines the switch side of the static external connection link</p> <p>Appears in: - ConnStaticExternalLink</p> Field Description <code>port</code> string Port defines the full name of the switch port in the format of \"device/port\", such as \"spine-1/Ethernet1\". SONiC port name is used as a port name and switch name should be same as the name of the Switch object. <code>ip</code> string IP is the IP address of the switch side of the static external connection link (switch port configuration) <code>nextHop</code> string NextHop is the next hop IP address for static routes that will be created for the subnets <code>subnets</code> string array Subnets is the list of subnets that will get static routes using the specified next hop <code>vlan</code> integer VLAN is the optional VLAN ID to be configured on the switch port"},{"location":"reference/api/#connunbundled","title":"ConnUnbundled","text":"<p>ConnUnbundled defines the unbundled connection (no port channel, single server to a single switch with a single link)</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>link</code> ServerToSwitchLink Link is the server-to-switch link <code>mtu</code> integer MTU is the MTU to be configured on the switch port or port channel"},{"location":"reference/api/#connvpcloopback","title":"ConnVPCLoopback","text":"<p>ConnVPCLoopback defines the VPC loopback connection (multiple port pairs on a single switch) that enables automated workaround named \"VPC Loopback\" that allow to avoid switch hardware limitations and traffic going through CPU in some cases</p> <p>Appears in: - ConnectionSpec</p> Field Description <code>links</code> SwitchToSwitchLink array Links is the list of VPC loopback links"},{"location":"reference/api/#connection","title":"Connection","text":"<p>Connection object represents a logical and physical connections between any devices in the Fabric (Switch, Server and External objects). It's needed to define all physical and logical connections between the devices in the Wiring Diagram. Connection type is defined by the top-level field in the ConnectionSpec. Exactly one of them could be used in a single Connection object.</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>Connection</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ConnectionSpec Spec is the desired state of the Connection <code>status</code> ConnectionStatus Status is the observed state of the Connection"},{"location":"reference/api/#connectionspec","title":"ConnectionSpec","text":"<p>ConnectionSpec defines the desired state of Connection</p> <p>Appears in: - AgentSpec - Connection</p> Field Description <code>unbundled</code> ConnUnbundled Unbundled defines the unbundled connection (no port channel, single server to a single switch with a single link) <code>bundled</code> ConnBundled Bundled defines the bundled connection (port channel, single server to a single switch with multiple links) <code>management</code> ConnMgmt Management defines the management connection (single control node/server to a single switch with a single link) <code>mclag</code> ConnMCLAG MCLAG defines the MCLAG connection (port channel, single server to pair of switches with multiple links) <code>eslag</code> ConnESLAG ESLAG defines the ESLAG connection (port channel, single server to 2-4 switches with multiple links) <code>mclagDomain</code> ConnMCLAGDomain MCLAGDomain defines the MCLAG domain connection which makes two switches into a single logical switch for server multi-homing <code>fabric</code> ConnFabric Fabric defines the fabric connection (single spine to a single leaf with at least one link) <code>vpcLoopback</code> ConnVPCLoopback VPCLoopback defines the VPC loopback connection (multiple port pairs on a single switch) for automated workaround <code>external</code> ConnExternal External defines the external connection (single switch to a single external device with a single link) <code>staticExternal</code> ConnStaticExternal StaticExternal defines the static external connection (single switch to a single external device with a single link)"},{"location":"reference/api/#connectionstatus","title":"ConnectionStatus","text":"<p>ConnectionStatus defines the observed state of Connection</p> <p>Appears in: - Connection</p>"},{"location":"reference/api/#fabriclink","title":"FabricLink","text":"<p>FabricLink defines the fabric connection link</p> <p>Appears in: - ConnFabric</p> Field Description <code>spine</code> ConnFabricLinkSwitch Spine is the spine side of the fabric link <code>leaf</code> ConnFabricLinkSwitch Leaf is the leaf side of the fabric link"},{"location":"reference/api/#location","title":"Location","text":"<p>Location defines the geopraphical position of the device in a datacenter</p> <p>Appears in: - SwitchSpec</p> Field Description <code>location</code> string <code>aisle</code> string <code>row</code> string <code>rack</code> string <code>slot</code> string"},{"location":"reference/api/#locationsig","title":"LocationSig","text":"<p>LocationSig contains signatures for the location UUID as well as the device location itself</p> <p>Appears in: - SwitchSpec</p> Field Description <code>sig</code> string <code>uuidSig</code> string"},{"location":"reference/api/#rack","title":"Rack","text":"<p>Rack is the Schema for the racks API</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>Rack</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RackSpec <code>status</code> RackStatus"},{"location":"reference/api/#rackposition","title":"RackPosition","text":"<p>RackPosition defines the geopraphical position of the rack in a datacenter</p> <p>Appears in: - RackSpec</p> Field Description <code>location</code> string <code>aisle</code> string <code>row</code> string"},{"location":"reference/api/#rackspec","title":"RackSpec","text":"<p>RackSpec defines the properties of a rack which we are modelling</p> <p>Appears in: - Rack</p> Field Description <code>numServers</code> integer <code>hasControlNode</code> boolean <code>hasConsoleServer</code> boolean <code>position</code> RackPosition"},{"location":"reference/api/#rackstatus","title":"RackStatus","text":"<p>RackStatus defines the observed state of Rack</p> <p>Appears in: - Rack</p>"},{"location":"reference/api/#server","title":"Server","text":"<p>Server is the Schema for the servers API</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>Server</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ServerSpec Spec is desired state of the server <code>status</code> ServerStatus Status is the observed state of the server"},{"location":"reference/api/#serverfacingconnectionconfig","title":"ServerFacingConnectionConfig","text":"<p>ServerFacingConnectionConfig defines any server-facing connection (unbundled, bundled, mclag, etc.) configuration</p> <p>Appears in: - ConnBundled - ConnESLAG - ConnMCLAG - ConnUnbundled</p> Field Description <code>mtu</code> integer MTU is the MTU to be configured on the switch port or port channel"},{"location":"reference/api/#serverspec","title":"ServerSpec","text":"<p>ServerSpec defines the desired state of Server</p> <p>Appears in: - Server</p> Field Description <code>type</code> ServerType Type is the type of server, could be control for control nodes or default (empty string) for everything else <code>description</code> string Description is a description of the server <code>profile</code> string Profile is the profile of the server, name of the ServerProfile object to be used for this server, currently not used by the Fabric"},{"location":"reference/api/#serverstatus","title":"ServerStatus","text":"<p>ServerStatus defines the observed state of Server</p> <p>Appears in: - Server</p>"},{"location":"reference/api/#servertoswitchlink","title":"ServerToSwitchLink","text":"<p>ServerToSwitchLink defines the server-to-switch link</p> <p>Appears in: - ConnBundled - ConnESLAG - ConnMCLAG - ConnUnbundled</p> Field Description <code>server</code> BasePortName Server is the server side of the connection <code>switch</code> BasePortName Switch is the switch side of the connection"},{"location":"reference/api/#servertype","title":"ServerType","text":"<p>Underlying type: string</p> <p>ServerType is the type of server, could be control for control nodes or default (empty string) for everything else</p> <p>Appears in: - ServerSpec</p>"},{"location":"reference/api/#switch","title":"Switch","text":"<p>Switch is the Schema for the switches API   All switches should always have 1 labels defined: wiring.githedgehog.com/rack. It represents name of the rack it belongs to.</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>Switch</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> SwitchSpec Spec is desired state of the switch <code>status</code> SwitchStatus Status is the observed state of the switch"},{"location":"reference/api/#switchgroup","title":"SwitchGroup","text":"<p>SwitchGroup is the marker API object to group switches together, switch can belong to multiple groups</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>SwitchGroup</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> SwitchGroupSpec Spec is the desired state of the SwitchGroup <code>status</code> SwitchGroupStatus Status is the observed state of the SwitchGroup"},{"location":"reference/api/#switchgroupspec","title":"SwitchGroupSpec","text":"<p>SwitchGroupSpec defines the desired state of SwitchGroup</p> <p>Appears in: - SwitchGroup</p>"},{"location":"reference/api/#switchgroupstatus","title":"SwitchGroupStatus","text":"<p>SwitchGroupStatus defines the observed state of SwitchGroup</p> <p>Appears in: - SwitchGroup</p>"},{"location":"reference/api/#switchredundancy","title":"SwitchRedundancy","text":"<p>SwitchRedundancy is the switch redundancy configuration which includes name of the redundancy group switch belongs to and its type, used both for MCLAG and ESLAG connections. It defines how redundancy will be configured and handled on the switch as well as which connection types will be available. If not specified, switch will not be part of any redundancy group. If name isn't empty, type must be specified as well and name should be the same as one of the SwitchGroup objects.</p> <p>Appears in: - SwitchSpec</p> Field Description <code>group</code> string Group is the name of the redundancy group switch belongs to <code>type</code> RedundancyType Type is the type of the redundancy group, could be mclag or eslag"},{"location":"reference/api/#switchrole","title":"SwitchRole","text":"<p>Underlying type: string</p> <p>SwitchRole is the role of the switch, could be spine, server-leaf or border-leaf or mixed-leaf</p> <p>Appears in: - AgentSpec - SwitchSpec</p>"},{"location":"reference/api/#switchspec","title":"SwitchSpec","text":"<p>SwitchSpec defines the desired state of Switch</p> <p>Appears in: - AgentSpec - Switch</p> Field Description <code>role</code> SwitchRole Role is the role of the switch, could be spine, server-leaf or border-leaf or mixed-leaf <code>description</code> string Description is a description of the switch <code>profile</code> string Profile is the profile of the switch, name of the SwitchProfile object to be used for this switch, currently not used by the Fabric <code>location</code> Location Location is the location of the switch, it is used to generate the location UUID and location signature <code>locationSig</code> LocationSig LocationSig is the location signature for the switch <code>groups</code> string array Groups is a list of switch groups the switch belongs to <code>redundancy</code> SwitchRedundancy Redundancy is the switch redundancy configuration including name of the redundancy group switch belongs to and its type, used both for MCLAG and ESLAG connections <code>vlanNamespaces</code> string array VLANNamespaces is a list of VLAN namespaces the switch is part of, their VLAN ranges could not overlap <code>asn</code> integer ASN is the ASN of the switch <code>ip</code> string IP is the IP of the switch that could be used to access it from other switches and control nodes in the Fabric <code>vtepIP</code> string VTEPIP is the VTEP IP of the switch <code>protocolIP</code> string ProtocolIP is used as BGP Router ID for switch configuration <code>portGroupSpeeds</code> object (keys:string, values:string) PortGroupSpeeds is a map of port group speeds, key is the port group name, value is the speed, such as '\"2\": 10G' <code>portSpeeds</code> object (keys:string, values:string) PortSpeeds is a map of port speeds, key is the port name, value is the speed <code>portBreakouts</code> object (keys:string, values:string) PortBreakouts is a map of port breakouts, key is the port name, value is the breakout configuration, such as \"1/55: 4x25G\""},{"location":"reference/api/#switchstatus","title":"SwitchStatus","text":"<p>SwitchStatus defines the observed state of Switch</p> <p>Appears in: - Switch</p>"},{"location":"reference/api/#switchtoswitchlink","title":"SwitchToSwitchLink","text":"<p>SwitchToSwitchLink defines the switch-to-switch link</p> <p>Appears in: - ConnMCLAGDomain - ConnVPCLoopback</p> Field Description <code>switch1</code> BasePortName Switch1 is the first switch side of the connection <code>switch2</code> BasePortName Switch2 is the second switch side of the connection"},{"location":"reference/api/#vlannamespace","title":"VLANNamespace","text":"<p>VLANNamespace is the Schema for the vlannamespaces API</p> Field Description <code>apiVersion</code> string <code>wiring.githedgehog.com/v1alpha2</code> <code>kind</code> string <code>VLANNamespace</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> VLANNamespaceSpec Spec is the desired state of the VLANNamespace <code>status</code> VLANNamespaceStatus Status is the observed state of the VLANNamespace"},{"location":"reference/api/#vlannamespacespec","title":"VLANNamespaceSpec","text":"<p>VLANNamespaceSpec defines the desired state of VLANNamespace</p> <p>Appears in: - AgentSpec - VLANNamespace</p> Field Description <code>ranges</code> VLANRange array Ranges is a list of VLAN ranges to be used in this namespace, couldn't overlap between each other and with Fabric reserved VLAN ranges"},{"location":"reference/api/#vlannamespacestatus","title":"VLANNamespaceStatus","text":"<p>VLANNamespaceStatus defines the observed state of VLANNamespace</p> <p>Appears in: - VLANNamespace</p>"},{"location":"reference/cli/","title":"Fabric CLI","text":"<p>Under construction.</p> <p>Currently Fabric CLI is represented by a kubectl plugin <code>kubectl-fabric</code> automatically installed on the Control Node. It is a wrapper around <code>kubectl</code> and Kubernetes client which allows to manage Fabric resources in a more convenient way. Fabric CLI only provides a subset of the functionality available via Fabric API and is focused on simplifying objects creation and some manipulation with the already existing objects while main get/list/update operations are expected to be done using <code>kubectl</code>.</p> <pre><code>core@control-1 ~ $ kubectl fabric\nNAME:\n   hhfctl - Hedgehog Fabric user client\n\nUSAGE:\n   hhfctl [global options] command [command options] [arguments...]\n\nVERSION:\n   v0.23.0\n\nCOMMANDS:\n   vpc                VPC commands\n   switch, sw, agent  Switch/Agent commands\n   connection, conn   Connection commands\n   switchgroup, sg    SwitchGroup commands\n   external           External commands\n   help, h            Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --verbose, -v  verbose output (includes debug) (default: true)\n   --help, -h     show help\n   --version, -V  print the version\n</code></pre>"},{"location":"reference/cli/#vpc","title":"VPC","text":"<p>Create VPC named <code>vpc-1</code> with subnet <code>10.0.1.0/24</code> and VLAN <code>1001</code> with DHCP enabled and DHCP range starting from <code>10.0.1.10</code> (optional):</p> <pre><code>core@control-1 ~ $ kubectl fabric vpc create --name vpc-1 --subnet 10.0.1.0/24 --vlan 1001 --dhcp --dhcp-start 10.0.1.10\n</code></pre> <p>Attach previously created VPC to the server <code>server-01</code> (which is connected to the Fabric using the <code>server-01--mclag--leaf-01--leaf-02</code> Connection):</p> <pre><code>core@control-1 ~ $ kubectl fabric vpc attach --vpc-subnet vpc-1/default --connection server-01--mclag--leaf-01--leaf-02\n</code></pre> <p>To peer VPC with another VPC (e.g. <code>vpc-2</code>) use the following command:</p> <pre><code>core@control-1 ~ $ kubectl fabric vpc peer --vpc vpc-1 --vpc vpc-2\n</code></pre>"},{"location":"release-notes/","title":"Release notes","text":""},{"location":"release-notes/#alpha-3","title":"Alpha-3","text":""},{"location":"release-notes/#sonic-support","title":"SONiC support","text":"<p>Broadcom Enterprise SONiC 4.2.0 (previously 4.1.1)</p>"},{"location":"release-notes/#multiple-ipv4-namespaces","title":"Multiple IPv4 namespaces","text":"<ul> <li>Support for multiple overlapping IPv4 addresses in the Fabric</li> <li>Integrated with on-demand DHCP Service (see below)</li> <li>All IPv4 addresses within a given VPC must be unique</li> <li>Only VPCs with non-overlapping IPv4 subnets can peer within the Fabric</li> <li>An external NAT device is required for peering of VPCs with overlapping subnets</li> </ul>"},{"location":"release-notes/#hedgehog-fabric-dhcp-and-ipam-service","title":"Hedgehog Fabric DHCP and IPAM Service","text":"<ul> <li>Custom DHCP server executing in the controllers</li> <li>Multiple IPv4 namespaces with overlapping subnets</li> <li>Multiple VLAN namespaces with overlapping VLAN ranges</li> <li>DHCP leases exposed through the Fabric API</li> <li>Available for VLAB as well as the Fabric</li> </ul>"},{"location":"release-notes/#hedgehog-fabric-ntp-service","title":"Hedgehog Fabric NTP Service","text":"<ul> <li>Custom NTP servers at the controller</li> <li>Switches automatically configured to use control node as NTP server</li> <li>NTP servers can be configured to sync to external time/NTP server</li> </ul>"},{"location":"release-notes/#staticexternal-connections","title":"StaticExternal connections","text":"<ul> <li>Directly connect external infrastructure services (such as NTP, DHCP, DNS) to the Fabric</li> <li>No BGP is required, just automatically configured  static routes</li> </ul>"},{"location":"release-notes/#dhcp-relay-to-3rd-party-dhcp-service","title":"DHCP Relay to 3rd party DHCP service","text":"<p>Support for 3rd party DHCP server (DHCP Relay config) through the API</p>"},{"location":"release-notes/#alpha-2","title":"Alpha-2","text":""},{"location":"release-notes/#controller","title":"Controller","text":"<p>A single controller. No controller redundancy.</p>"},{"location":"release-notes/#controller-connectivity","title":"Controller connectivity","text":"<p>For CLOS/LEAF-SPINE fabrics, it is recommended that the controller connects to one or more leaf switches in the fabric on front-facing data ports. Connection to two or more leaf switches is recommended for redundancy and performance. No port break-out functionality is supported for controller connectivity.</p> <p>Spine controller connectivity is not supported.</p> <p>For Collapsed Core topology, the controller can connect on front-facing data ports, as described above, or on management ports. Note that every switch in the collapsed core topology must be connected to the controller.</p> <p>Management port connectivity can also be supported for CLOS/LEAF-SPINE topology but requires all switches connected to the controllers via management ports. No chain booting is possible for this configuration.</p>"},{"location":"release-notes/#controller-requirements","title":"Controller requirements","text":"<ul> <li>One  1 gig+ port per to connect to each controller attached switch</li> <li>One+ 1 gig+ ports connecting to the external management network.</li> <li>4 Cores, 12GB RAM, 100GB SSD.</li> </ul>"},{"location":"release-notes/#chain-booting","title":"Chain booting","text":"<p>Switches not directly connecting to the controllers can chain boot via the data network.</p>"},{"location":"release-notes/#topology-support","title":"Topology support","text":"<p>CLOS/LEAF-SPINE and Collapsed Core topologies are supported.</p>"},{"location":"release-notes/#leaf-roles-for-clos-topology","title":"LEAF Roles for CLOS topology","text":"<p>server leaf, border leaf, and mixed leaf modes are supported.</p>"},{"location":"release-notes/#collapsed-core-topology","title":"Collapsed Core Topology","text":"<p>Two ToR/LEAF switches with MCLAG server connection.</p>"},{"location":"release-notes/#server-multihoming","title":"Server multihoming","text":"<p>MCLAG-only.</p>"},{"location":"release-notes/#device-support","title":"Device support","text":""},{"location":"release-notes/#leafs","title":"LEAFs","text":"<ul> <li>DELL:</li> <li>S5248F-ON</li> <li> <p>S5232F-ON</p> </li> <li> <p>Edge-Core:</p> </li> <li>DCS204 (AS7726-32X)</li> <li>DCS203 (AS7326-56X)</li> <li>EPS203 (AS4630-54NPE)</li> </ul>"},{"location":"release-notes/#spines","title":"SPINEs","text":"<ul> <li>DELL:</li> <li>S5232F-ON</li> <li>Edge-Core:</li> <li>DCS204 (AS7726-32X)</li> </ul>"},{"location":"release-notes/#underlay-configuration","title":"Underlay configuration:","text":"<p>Port speed, port group speed, port breakouts are configurable through the API</p>"},{"location":"release-notes/#vpc-overlay-implementation","title":"VPC (overlay) Implementation","text":"<p>VXLAN-based BGP eVPN.</p>"},{"location":"release-notes/#multi-subnet-vpcs","title":"Multi-subnet VPCs","text":"<p>A VPC consists of subnets, each with a user-specified VLAN for external host/server connectivity.</p>"},{"location":"release-notes/#multiple-ip-address-namespaces","title":"Multiple IP address namespaces","text":"<p>Multiple IP address namespaces are supported per fabric. Each VPC belongs to the corresponding IPv4 namespace. There are no subnet overlaps within a single IPv4 namespace. IP address namespaces can mutually overlap.</p>"},{"location":"release-notes/#vlan-namespace","title":"VLAN Namespace","text":"<p>VLAN Namespaces guarantee the uniqueness of VLANs for a set of participating devices. Each switch belongs to a list of VLAN namespaces with non-overlapping VLAN ranges. Each VPC belongs to the VLAN namespace. There are no VLAN overlaps within a single VLAN namespace.</p> <p>This feature is useful when multiple VM-management domains (like separate VMware clusters connect to the fabric).</p>"},{"location":"release-notes/#switch-groups","title":"Switch Groups","text":"<p>Each switch belongs to a list of switch groups used for identifying redundancy groups for things like external connectivity.</p>"},{"location":"release-notes/#mutual-vpc-peering","title":"Mutual VPC Peering","text":"<p>VPC peering is supported and possible between a pair of VPCs that belong to the same IPv4 and VLAN namespaces.</p>"},{"location":"release-notes/#external-vpc-peering","title":"External VPC Peering","text":"<p>VPC peering provides the means of peering with external networking devices (edge routers, firewalls, or data center interconnects). VPC egress/ingress is pinned to a specific group of the border or mixed leaf switches. Multiple \u201cexternal systems\u201d with multiple devices/links in each of them are supported.</p> <p>The user controls what subnets/prefixes to import and export from/to the external system.</p> <p>No NAT function is supported for external peering.</p>"},{"location":"release-notes/#host-connectivity","title":"Host connectivity","text":"<p>Servers can be attached as Unbundled, Bundled (LAG) and MCLAG</p>"},{"location":"release-notes/#dhcp-service","title":"DHCP Service","text":"<p>VPC is provided with an optional DHCP service with simple IPAM</p>"},{"location":"release-notes/#local-vpc-peering-loopbacks","title":"Local VPC peering loopbacks","text":"<p>To enable local inter-vpc peering that allows routing of traffic between VPCs, local loopbacks are required to overcome silicon limitations.</p>"},{"location":"release-notes/#scale","title":"Scale","text":"<ul> <li>Maximum fabric size: 20 LEAF/ToR switches.</li> <li>Routes per switch: 64k</li> <li>[ silicon platform limitation in Trident 3; limits te number of endpoints in the fabric  ]</li> <li>Total VPCs per switch: up to 1000</li> <li>[ Including VPCs attached at the given switch and VPCs peered with ]</li> <li>Total VPCs per VLAN namespace: up to 3000</li> <li>[ assuming 1 subnet per VPC ]</li> <li>Total VPCs per fabric:  unlimited</li> <li>[ if using multiple VLAN namespaces ]</li> <li>VPC subnets per switch: up to 3000</li> <li>VPC subnets per VLAN namespace up to 3000</li> <li>Subnets per VPC: up to 20</li> <li>[ just a validation; the current design allows up to 100, but it could be increased even more in the future ]</li> <li>VPC Slots per remote peering @ switch: 2</li> <li>Max VPC loopbacks per switch: 500</li> <li>[ VPC loopback workarounds per switch are needed for local peering when both VPCs are attached to the switch or for external peering with VPC attached on the same switch that is peering with external ]</li> </ul>"},{"location":"release-notes/#software-versions","title":"Software versions","text":"<ul> <li>Fabric: v0.23.0</li> <li>Das-boot: v0.11.4</li> <li>Fabricator: v0.8.0</li> <li>K3s: v1.27.4-k3s1</li> <li>Zot: v1.4.3</li> <li>SONiC</li> <li>Broadcom Enterprise Base 4.1.1</li> <li>Broadcom Enterprise Campus 4.1.1</li> </ul>"},{"location":"release-notes/#known-limitations","title":"Known Limitations","text":"<ul> <li>MTU setting inflexibility:</li> <li>Fabric MTU is 9100 and not configurable right now (A3 planned)</li> <li>Server-facing MTU is 9136 and not configurable right now (A3+)</li> <li>no support for Access VLANs for attaching servers (A3 planned)</li> <li>VPC peering is enabled on all subnets of the participating VPCs. No subnet selection for peering. (A3 planned)</li> <li>peering with external is only possible with a VLAN (by design)</li> <li>If you have VPCs with remote peering on a switch group, you can\u2019t attach those VPCs on that switch group (by definition of remote peering)</li> <li>if a group of VPCs has remote peering on a switch group, any other VPC that will peer with those VPCs remotely will need to use the same switch group (by design)</li> <li>if VPC peers with external, it can only be remotely peered with on the same switches that have a connection to that external (by design)</li> <li>the server-facing connection object is immutable as it\u2019s very easy to get into a deadlock, re-create to change it (A3+)</li> </ul>"},{"location":"release-notes/#alpha-1","title":"Alpha-1","text":"<ul> <li> <p>Controller:</p> <ul> <li>A single controller connecting to each switch management port. No redundancy.</li> </ul> </li> <li> <p>Controller requirements:</p> <ul> <li>One 1 gig port per switch</li> <li>One+ 1 gig+ ports connecting to the external management network.</li> <li>4 Cores, 12GB RAM, 100GB SSD.</li> </ul> </li> <li> <p>Seeder:</p> <ul> <li>Seeder and Controller functions co-resident on the control node. Switch booting and ZTP on management ports directly connected to the controller.</li> </ul> </li> <li> <p>HHFab - the fabricator:</p> <ul> <li>An operational tool to generate, initiate, and maintain the fabric software appliance.  Allows fabrication of the environment-specific image with all of the required underlay and security configuration baked in.</li> </ul> </li> <li> <p>DHCP Service:</p> <ul> <li>A simple DHCP server for assigning IP addresses to hosts connecting to the fabric, optimized for use with VPC overlay.</li> </ul> </li> <li> <p>Topology:</p> <ul> <li>Support for a Collapsed Core topology with 2 switch nodes.</li> </ul> </li> <li> <p>Underlay:</p> <ul> <li>A simple single-VRF network with a BGP control plane.  IPv4 support only.</li> </ul> </li> <li> <p>External connectivity:</p> <ul> <li>An edge router must be connected to selected ports of one or both switches.  IPv4 support only.</li> </ul> </li> <li> <p>Dual-homing:</p> <ul> <li>L2 Dual homing with MCLAG is implemented to connect servers, storage, and other devices in the data center.  NIC bonding and LACP configuration at the host are required.</li> </ul> </li> <li> <p>VPC overlay implementation:</p> <ul> <li>VPC is implemented as a set of ACLs within the underlay VRF. External connectivity to the VRF is performed via internally managed VLANs.  IPv4 support only.</li> </ul> </li> <li> <p>VPC Peering:</p> <ul> <li>VPC peering is performed via ACLs with no fine-grained control.</li> </ul> </li> <li> <p>NAT</p> <ul> <li>DNAT + SNAT are supported per VPC. SNAT and DNAT can\u2019t be enabled per VPC simultaneously.</li> </ul> </li> <li> <p>Hardware support:</p> <ul> <li>Please see the supported hardware list.</li> </ul> </li> <li> <p>Virtual Lab:</p> <ul> <li>A simulation of the two-node Collapsed Core Topology as a virtual environment. Designed for use as a network simulation, a configuration scratchpad, or a training/demonstration tool.  Minimum requirements: 8 cores, 24GB RAM, 100GB SSD</li> </ul> </li> <li> <p>Limitations:</p> <ul> <li>40 VPCs max</li> <li>50 VPC peerings</li> <li>[ 768 ACL entry platform limitation from Broadcom ]</li> </ul> </li> <li> <p>Software versions:</p> <ul> <li>Fabricator: v0.5.2</li> <li>Fabric: v0.18.6</li> <li>Das-boot: v0.8.2</li> <li>K3s: v1.27.4-k3s1</li> <li>Zot: v1.4.3</li> <li>SONiC: Broadcom Enterprise Base 4.1.1</li> </ul> </li> </ul>"},{"location":"troubleshooting/overview/","title":"Troubleshooting","text":"<p>Under construction.</p>"},{"location":"user-guide/connections/","title":"Connections","text":"<p>The <code>Connection</code> object represents a logical and physical connections between any devices in the Fabric (<code>Switch</code>, <code>Server</code> and <code>External</code> objects). It's needed to define all connections between the devices in the Wiring Diagram.</p> <p>There are multiple types of connections.</p>"},{"location":"user-guide/connections/#server-connections-user-facing","title":"Server connections (user-facing)","text":"<p>Server connections are used to connect workload servers to the switches.</p>"},{"location":"user-guide/connections/#unbundled","title":"Unbundled","text":"<p>Unbundled server connections are used to connect servers to the single switche using a single port.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: server-4--unbundled--s5248-02\n  namespace: default\nspec:\n  unbundled:\n    link: # Defines a single link between a server and a switch\n      server:\n        port: server-4/enp2s1\n      switch:\n        port: s5248-02/Ethernet3\n</code></pre>"},{"location":"user-guide/connections/#bundled","title":"Bundled","text":"<p>Bundled server connections are used to connect servers to the single switch using multiple ports (port channel, LAG).</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: server-3--bundled--s5248-01\n  namespace: default\nspec:\n  bundled:\n    links: # Defines multiple links between a single server and a single switch\n    - server:\n        port: server-3/enp2s1\n      switch:\n        port: s5248-01/Ethernet3\n    - server:\n        port: server-3/enp2s2\n      switch:\n        port: s5248-01/Ethernet4\n</code></pre>"},{"location":"user-guide/connections/#mclag","title":"MCLAG","text":"<p>MCLAG server connections are used to connect servers to the pair of switches using multiple ports (Dual-homing). Switches should be configured as an MCLAG pair which requires them to be in a single redundancy group of type <code>mclag</code> and Connection with type <code>mclag-domain</code> between them. MCLAG switches should also have the same <code>spec.ASN</code> and <code>spec.VTEPIP</code>.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: server-1--mclag--s5248-01--s5248-02\n  namespace: default\nspec:\n  mclag:\n    links: # Defines multiple links between a single server and a pair of switches\n    - server:\n        port: server-1/enp2s1\n      switch:\n        port: s5248-01/Ethernet1\n    - server:\n        port: server-1/enp2s2\n      switch:\n        port: s5248-02/Ethernet1\n</code></pre>"},{"location":"user-guide/connections/#eslag","title":"ESLAG","text":"<p>ESLAG server connections are used to connect servers to the 2-4 switches using multiple ports (Multi-homing). Switches should belong to the same redundancy group with type <code>eslag</code>, but no other configuration like in MCLAG case is required.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: server-1--eslag--s5248-01--s5248-02\n  namespace: default\nspec:\n  eslag:\n    links: # Defines multiple links between a single server and a 2-4 switches\n    - server:\n        port: server-1/enp2s1\n      switch:\n        port: s5248-01/Ethernet1\n    - server:\n        port: server-1/enp2s2\n      switch:\n        port: s5248-02/Ethernet1\n</code></pre>"},{"location":"user-guide/connections/#switch-connections-fabric-facing","title":"Switch connections (fabric-facing)","text":"<p>Switch connections are used to connect switches to each other and provide any needed \"service\" connectivity to implement the Fabric features.</p>"},{"location":"user-guide/connections/#fabric","title":"Fabric","text":"<p>Connections between specific spine and leaf, covers all actual wires between a single pair.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: s5232-01--fabric--s5248-01\n  namespace: default\nspec:\n  fabric:\n    links: # Defines multiple links between a spine-leaf pair of switches with IP addresses\n    - leaf:\n        ip: 172.30.30.1/31\n        port: s5248-01/Ethernet48\n      spine:\n        ip: 172.30.30.0/31\n        port: s5232-01/Ethernet0\n    - leaf:\n        ip: 172.30.30.3/31\n        port: s5248-01/Ethernet56\n      spine:\n        ip: 172.30.30.2/31\n        port: s5232-01/Ethernet4\n</code></pre>"},{"location":"user-guide/connections/#mclag-domain","title":"MCLAG-Domain","text":"<p>Used to define a pair of MCLAG switches with Session and Peer link between them. Switches should be configured as an MCLAG pair which requires them to be in a single redundancy group of type <code>mclag</code> and Connection with type <code>mclag-domain</code> between them. MCLAG switches should also have the same <code>spec.ASN</code> and <code>spec.VTEPIP</code>.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: s5248-01--mclag-domain--s5248-02\n  namespace: default\nspec:\n  mclagDomain:\n    peerLinks: # Defines multiple links between a pair of MCLAG switches for Peer link\n    - switch1:\n        port: s5248-01/Ethernet72\n      switch2:\n        port: s5248-02/Ethernet72\n    - switch1:\n        port: s5248-01/Ethernet73\n      switch2:\n        port: s5248-02/Ethernet73\n    sessionLinks: # Defines multiple links between a pair of MCLAG switches for Session link\n    - switch1:\n        port: s5248-01/Ethernet74\n      switch2:\n        port: s5248-02/Ethernet74\n    - switch1:\n        port: s5248-01/Ethernet75\n      switch2:\n        port: s5248-02/Ethernet75\n</code></pre>"},{"location":"user-guide/connections/#vpc-loopback","title":"VPC-Loopback","text":"<p>Required to implement a workaround for the local VPC peering (when both VPC are attached to the same switch) which is caused by the hardware limitation of the currently supported switches.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: s5248-01--vpc-loopback\n  namespace: default\nspec:\n  vpcLoopback:\n    links: # Defines multiple loopbacks on a single switch\n    - switch1:\n        port: s5248-01/Ethernet16\n      switch2:\n        port: s5248-01/Ethernet17\n    - switch1:\n        port: s5248-01/Ethernet18\n      switch2:\n        port: s5248-01/Ethernet19\n</code></pre>"},{"location":"user-guide/connections/#management","title":"Management","text":"<p>Connection to the Control Node.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: control-1--mgmt--s5248-01-front\n  namespace: default\nspec:\n  management:\n    link: # Defines a single link between a control node and a switch\n      server:\n        ip: 172.30.20.0/31\n        port: control-1/enp2s1\n      switch:\n        ip: 172.30.20.1/31\n        port: s5248-01/Ethernet0\n</code></pre>"},{"location":"user-guide/connections/#connecting-fabric-to-outside-world","title":"Connecting Fabric to outside world","text":"<p>Provides connectivity to the outside world, e.g. internet, other networks or some other systems such as DHCP, NTP, LMA, AAA services.</p>"},{"location":"user-guide/connections/#staticexternal","title":"StaticExternal","text":"<p>Simple way to connect things like DHCP server directly to the Fabric by connecting it to specific switch ports.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: third-party-dhcp-server--static-external--s5248-04\n  namespace: default\nspec:\n  staticExternal:\n    link:\n      switch:\n        port: s5248-04/Ethernet1 # switch port to use\n        ip: 172.30.50.5/24 # IP address that will be assigned to the switch port\n        vlan: 1005 # Optional VLAN ID to use for the switch port, if 0 - no VLAN is configured\n        subnets: # List of subnets that will be routed to the switch port using static routes and next hop\n          - 10.99.0.1/24\n          - 10.199.0.100/32\n        nextHop: 172.30.50.1 # Next hop IP address that will be used when configuring static routes for the \"subnets\" list\n</code></pre>"},{"location":"user-guide/connections/#external","title":"External","text":"<p>Connection to the external systems, e.g. edge/provider routers using BGP peering and configuring Inbound/Outbound communities as well as granularly controlling what's getting advertised and which routes are accepted.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: s5248-03--external--5835\n  namespace: default\nspec:\n  external:\n    link: # Defines a single link between a switch and an external system\n      switch:\n        port: s5248-03/Ethernet3\n</code></pre>"},{"location":"user-guide/devices/","title":"Switches and Servers","text":"<p>All devices in the Hedgehog Fabric are divided into two groups: switches and servers and represented by corresponding <code>Switch</code> and <code>Server</code> objects in the API. It's needed to define all participants of the Fabric and their roles in the Wiring Diagram as well as Connections between them.</p>"},{"location":"user-guide/devices/#switches","title":"Switches","text":"<p>Switches are the main building blocks of the Fabric. They are represented by <code>Switch</code> objects in the API and consists of the basic information like name, description, location, role, etc. as well as port group speeds, port breakouts, ASN, IP addresses and etc.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Switch\nmetadata:\n  name: s5248-01\n  namespace: default\nspec:\n  asn: 65101 # ASN of the switch\n  description: leaf-1\n  ip: 172.30.10.100/32 # Switch IP that will be accessible from the Control Node\n  location:\n    location: gen--default--s5248-01\n  locationSig:\n    sig: &lt;undefined&gt;\n    uuidSig: &lt;undefined&gt;\n  portBreakouts: # Configures port breakouts for the switch\n    1/55: 4x25G\n  portGroupSpeeds: # Configures port group speeds for the switch\n    \"1\": 10G\n    \"2\": 10G\n  protocolIP: 172.30.11.100/32 # Used as BGP router ID\n  role: server-leaf # Role of the switch, one of server-leaf, border-leaf and mixed-leaf\n  vlanNamespaces: # Defines which VLANs could be used to attach servers\n  - default\n  vtepIP: 172.30.12.100/32\n  groups: # Defines which groups the switch belongs to\n  - some-group\n</code></pre> <p>The <code>SwitchGroup</code> is just a marker at that point and doesn't have any configuration options.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: SwitchGroup\nmetadata:\n  name: border\n  namespace: default\nspec: {}\n</code></pre>"},{"location":"user-guide/devices/#servers","title":"Servers","text":"<p>It includes both control nodes and user's workload servers.</p> <p>Control Node:</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Server\nmetadata:\n  name: control-1\n  namespace: default\nspec:\n  type: control # Type of the server, one of control or \"\" (empty) for regular workload server\n</code></pre> <p>Regular workload server:</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Server\nmetadata:\n  name: server-1\n  namespace: default\nspec:\n  description: MH s5248-01/E1 s5248-02/E1\n</code></pre>"},{"location":"user-guide/external/","title":"External Peering","text":"<p>Hedgehog Fabric uses Border Leaf concept to exchange VPC routes outside the Fabric and providing L3 connectivity. <code>External Peering</code> feature allows to set up an external peering endpoint and to enforce several policies between internal and external endpoints.</p> <p>Hedgehog Fabric does not operate Edge side devices.</p>"},{"location":"user-guide/external/#overview","title":"Overview","text":"<p>Traffic exit from the Fabric is done on Border Leafs that are connected with Edge devices. Border Leafs are suitable to terminate l2vpn connections and distinguish VPC L3 routable traffic towards Edge device as well as to land VPC servers. Border Leafs (or Borders) can connect to several Edge devices.</p> <p>External Peering is only available on the switch devices that are capable for sub-interfaces.</p>"},{"location":"user-guide/external/#connect-border-leaf-to-edge-device","title":"Connect Border Leaf to Edge device","text":"<p>In order to distinguish VPC traffic Edge device should be capable for - Set up BGP IPv4 to advertise and receive routes from the Fabric - Connect to Fabric Border Leaf over Vlan - Be able to mark egress routes towards the Fabric with BGP Communities - Be able to filter ingress routes from the Fabric by BGP Communities</p> <p>All other filtering and processing of L3 Routed Fabric traffic should be done on the Edge devices.</p>"},{"location":"user-guide/external/#control-plane","title":"Control Plane","text":"<p>Fabric is sharing VPC routes with Edge devices via BGP. Peering is done over Vlan in IPv4 Unicast AFI/SAFI.</p>"},{"location":"user-guide/external/#data-plane","title":"Data Plane","text":"<p>VPC L3 routable traffic will be tagged with Vlan and sent to Edge device. Later processing of VPC traffic (NAT, PBR, etc) should happen on Edge devices.</p>"},{"location":"user-guide/external/#vpc-access-to-edge-device","title":"VPC access to Edge device","text":"<p>Each VPC within the Fabric can be allowed to access Edge devices. Additional filtering can be applied to the routes that VPC can export to Edge devices and import from the Edge devices.</p>"},{"location":"user-guide/external/#api-and-implementation","title":"API and implementation","text":""},{"location":"user-guide/external/#external","title":"External","text":"<p>General configuration starts with specification of <code>External</code> objects. Each object of <code>External</code> type can represent a set of Edge devices, or a single BGP instance on Edge device, or any other united Edge entities that can be described with following config</p> <ul> <li>Name of <code>External</code></li> <li>Inbound routes are marked with dedicated BGP community</li> <li>Outbound routes are required to be marked with dedicated community</li> </ul> <p>Each <code>External</code> should be bound to some VPC IP Namespace, otherwise prefixes overlap may happen.</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: External\nmetadata:\n  name: default--5835\nspec:\n  ipv4Namespace: # VPC IP Namespace\n  inboundCommunity: # BGP Standard Community of routes from Edge devices\n  outboundCommunity: # BGP Standard Community required to be assigned on prefixes advertised from Fabric\n</code></pre>"},{"location":"user-guide/external/#connection","title":"Connection","text":"<p><code>Connection</code> of type <code>external</code> is used to identify switch port on Border leaf that is cabled with an Edge device.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: # specified or generated\nspec:\n  external:\n    link:\n      switch:\n        port: # SwtichName/EthernetXXX\n</code></pre>"},{"location":"user-guide/external/#external-attachment","title":"External Attachment","text":"<p><code>External Attachment</code> is a definition of BGP Peering and traffic connectivity between a Border leaf and <code>External</code>. Attachments are bound to <code>Connection</code> with type <code>external</code> and specify <code>Vlan</code> that will be used to segregate particular Edge peering.</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: ExternalAttachment\nmetadata:\n  name: #\nspec:\n  connection: # Name of the Connection with type external\n  external: # Name of the External to pick config\n  neighbor:\n    asn: # Edge device ASN\n    ip: # IP address of Edge device to peer with\n  switch:\n    ip: # IP Address on the Border Leaf to set up BGP peering\n    vlan: # Vlan ID to tag control and data traffic\n</code></pre> <p>Several <code>External Attachment</code> can be configured for the same <code>Connection</code> but for different <code>vlan</code>.</p>"},{"location":"user-guide/external/#external-vpc-peering","title":"External VPC Peering","text":"<p>To allow specific VPC have access to Edge devices VPC should be bound to specific <code>External</code> object. This is done via <code>External Peering</code> object.</p> <p><pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: ExternalPeering\nmetadata:\n  name: # Name of ExternalPeering\nspec:\n  permit:\n    external:\n      name: # External Name\n      prefixes: # List of prefixes(routes) to be allowed to pick up from External\n      - # IPv4 Prefix\n    vpc:\n      name: # VPC Name\n      subnets: # List of VPC subnets name to be allowed to have access to External (Edge)\n      - # Name of the subnet within VPC\n</code></pre> <code>Prefixes</code> can be specified as <code>exact match</code> or with mask range indicators <code>le</code> and <code>ge</code> keywords. <code>le</code> is identifying prefixes lengths that are <code>less than or equal</code> and <code>ge</code> for prefixes lengths that are <code>greater than or equal</code>.</p> <p>Example: Allow ANY IPv4 prefix that came from <code>External</code> - allow all prefixes that match default route with any prefix length <pre><code>spec:\n  permit:\n    external:\n      name: ###\n      prefixes:\n      - le: 32\n        prefix: 0.0.0.0/0\n</code></pre> <code>ge</code> and <code>le</code> can also be combined.</p> <p>Example: <pre><code>spec:\n  permit:\n    external:\n      name: ###\n      prefixes:\n      - le: 24\n        ge: 16\n        prefix: 77.0.0.0/8\n</code></pre> For instance, <code>77.42.0.0/18</code> will be matched for given prefix rule above, but <code>77.128.77.128/25</code> or <code>77.10.0.0/16</code> won't.</p>"},{"location":"user-guide/external/#examples","title":"Examples","text":"<p>This example will show peering with <code>External</code> object with name <code>HedgeEdge</code> given Fabric VPC with name <code>vpc-1</code> on the Border Leaf <code>switchBorder</code> that has a cable between an Edge device on the port <code>Ethernet42</code>. <code>vpc-1</code> is required to receive any prefixes advertised from the <code>External</code>.</p>"},{"location":"user-guide/external/#fabric-api-configuration","title":"Fabric API configuration","text":""},{"location":"user-guide/external/#external_1","title":"External","text":"<p><pre><code># hhfctl external create --name HedgeEdge --ipns default --in 65102:5000 --out 5000:65102\n</code></pre> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: External\nmetadata:\n  name: HedgeEdge\n  namespace: default\nspec:\n  inboundCommunity: 65102:5000\n  ipv4Namespace: default\n  outboundCommunity: 5000:65102\n</code></pre></p>"},{"location":"user-guide/external/#connection_1","title":"Connection","text":"<p>Connection should be specified in the <code>wiring</code> diagram.</p> <pre><code>###\n### switchBorder--external--HedgeEdge\n###\napiVersion: wiring.githedgehog.com/v1alpha2\nkind: Connection\nmetadata:\n  name: switchBorder--external--HedgeEdge\nspec:\n  external:\n    link:\n      switch:\n        port: switchBorder/Ethernet42\n</code></pre>"},{"location":"user-guide/external/#externalattachment","title":"ExternalAttachment","text":"<p>Specified in <code>wiring</code> diagram <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: ExternalAttachment\nmetadata:\n  name: switchBorder--HedgeEdge\nspec:\n  connection: switchBorder--external--HedgeEdge\n  external: HedgeEdge\n  neighbor:\n    asn: 65102\n    ip: 100.100.0.6\n  switch:\n    ip: 100.100.0.1/24\n    vlan: 100\n</code></pre></p>"},{"location":"user-guide/external/#externalpeering","title":"ExternalPeering","text":"<pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: ExternalPeering\nmetadata:\n  name: vpc-1--HedgeEdge\nspec:\n  permit:\n    external:\n      name: HedgeEdge\n      prefixes:\n      - le: 32\n        prefix: 0.0.0.0/0\n    vpc:\n      name: vpc-1\n      subnets:\n      - default\n</code></pre>"},{"location":"user-guide/external/#example-edge-side-bgp-configuration-based-on-sonic-os","title":"Example Edge side BGP configuration based on SONiC OS","text":"<p>NOTE: Hedgehog does not recommend using following configuration for production. It's just as example of Edge Peer config</p> <p>Interface config <pre><code>interface Ethernet2.100\n encapsulation dot1q vlan-id 100\n description switchBorder--Ethernet42\n no shutdown\n ip vrf forwarding VrfHedge\n ip address 100.100.0.6/24\n</code></pre></p> <p>BGP Config <pre><code>!\nrouter bgp 65102 vrf VrfHedge\n log-neighbor-changes\n timers 60 180\n !\n address-family ipv4 unicast\n  maximum-paths 64\n  maximum-paths ibgp 1\n  import vrf VrfPublic\n !\n neighbor 100.100.0.1\n  remote-as 65103\n  !\n  address-family ipv4 unicast\n   activate\n   route-map HedgeIn in\n   route-map HedgeOut out\n   send-community both\n !\n</code></pre> Route Map configuration <pre><code>route-map HedgeIn permit 10\n match community Hedgehog\n!\nroute-map HedgeOut permit 10\n set community 65102:5000\n!\n\nbgp community-list standard HedgeIn permit 5000:65102\n</code></pre></p>"},{"location":"user-guide/harvester/","title":"Using VPCs with Harvester","text":"<p>It's an example of how Hedgehog Fabric can be used with Harvester or any hypervisor on the servers connected to Fabric. It assumes that you have already installed Fabric and have some servers running Harvester attached to it.</p> <p>You'll need to define <code>Server</code> object per each server running Harvester and <code>Connection</code> object per each server connection to the switches.</p> <p>You can have multiple VPCs created and attached to the <code>Connections</code> to this servers to make them available to the VMs in Harvester or any other hypervisor.</p>"},{"location":"user-guide/harvester/#configure-harvester","title":"Configure Harvester","text":""},{"location":"user-guide/harvester/#add-a-cluster-network","title":"Add a Cluster Network","text":"<p>From the \"Cluster Network/Confg\" side menu. Create a new Cluster Network.</p> <p></p> <p>Here is what the CRD looks like cleaned up:</p> <pre><code>apiVersion: network.harvesterhci.io/v1beta1\nkind: ClusterNetwork\nmetadata:\n  name: testnet\n</code></pre>"},{"location":"user-guide/harvester/#add-a-network-config","title":"Add a Network Config","text":"<p>By clicking \"Create Network Confg\". Add your connections and select bonding type.</p> <p></p> <p>The resulting cleaned up CRD:</p> <pre><code>apiVersion: network.harvesterhci.io/v1beta1\nkind: VlanConfig\nmetadata:\n  name: testconfig\n  labels:\n    network.harvesterhci.io/clusternetwork: testnet\nspec:\n  clusterNetwork: testnet\n  uplink:\n    bondOptions:\n      miimon: 100\n      mode: 802.3ad\n    linkAttributes:\n      txQLen: -1\n    nics:\n      - enp5s0f0\n      - enp3s0f1\n</code></pre>"},{"location":"user-guide/harvester/#add-vlan-based-vm-networks","title":"Add VLAN based VM Networks","text":"<p>Browse over to \"VM Networks\" and add one for each Vlan you want to support, assigning them to the cluster network.</p> <p> </p> <p>Here is what the CRDs will look like for both vlans:</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  labels:\n    network.harvesterhci.io/clusternetwork: testnet\n    network.harvesterhci.io/ready: 'true'\n    network.harvesterhci.io/type: L2VlanNetwork\n    network.harvesterhci.io/vlan-id: '1001'\n  name: testnet1001\n  namespace: default\nspec:\n  config: &gt;-\n    {\"cniVersion\":\"0.3.1\",\"name\":\"testnet1001\",\"type\":\"bridge\",\"bridge\":\"testnet-br\",\"promiscMode\":true,\"vlan\":1001,\"ipam\":{}}\n</code></pre> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: testnet1000\n  labels:\n    network.harvesterhci.io/clusternetwork: testnet\n    network.harvesterhci.io/ready: 'true'\n    network.harvesterhci.io/type: L2VlanNetwork\n    network.harvesterhci.io/vlan-id: '1000'\n    #  key: string\n  namespace: default\nspec:\n  config: &gt;-\n    {\"cniVersion\":\"0.3.1\",\"name\":\"testnet1000\",\"type\":\"bridge\",\"bridge\":\"testnet-br\",\"promiscMode\":true,\"vlan\":1000,\"ipam\":{}}\n</code></pre>"},{"location":"user-guide/harvester/#using-the-vpcs","title":"Using the VPCs","text":"<p>Now you can choose created VM Networks when creating a VM in Harvester and have them created as part of the VPC.</p>"},{"location":"user-guide/overview/","title":"Overview","text":"<p>The chapter is intended to give an overview of the main features of the Hedgehog Fabric and their usage.</p>"},{"location":"user-guide/shrink-expand/","title":"Fabric Shrink/Expand","text":"<p>This article gives brief overview of how to add or remove switches within the fabric using Hedgehog Fabric API and manage connections between them.</p> <p>Manipulating API objects done with assumption that target devices are correctly cabeled and connected.</p> <p>This article operates terms that can be found in Hedgehog Concepts, User Guide documentation and Fabric API reference.</p>"},{"location":"user-guide/shrink-expand/#add-switch-to-the-existing-fabric","title":"Add switch to the existing fabric","text":"<p>For every switch to be added into the Hedgehog fabric it should have a corresponding <code>Switch</code> object. Example can be found in User Guilde.</p> <p>If the<code>Switch</code> will be used in <code>ESLAG</code> or <code>MCLAG</code> groups, appropriate groups should exist. Redundancy groups should be specified in the <code>Switch</code> object before creation.</p> <p>After the <code>Switch</code> object is created, dedicated device <code>Connections</code> can be defined and created. Based on the <code>Switch</code> role given to the device types of connections may be different. Please refer to Connections section.</p> <p>If switch is facing control node connection on the front-panel port, such switch port should be described in <code>Management</code> connection</p> <p>Switch device should be booted in <code>ONIE</code> or <code>HONIE</code> Installation mode to install SONiC OS and configure Fabric agent</p>"},{"location":"user-guide/shrink-expand/#remove-switch-from-the-existing-fabric","title":"Remove switch from the existing fabric","text":"<p>If the switch has to be decommissioned or removed there are several preparation steps before disabling it from the Fabric.</p> <p>[!WARNING] Currently the <code>Wiring</code> diagram used for initial deployment is saved in <code>/var/lib/rancher/k3s/server/manifests/hh-wiring.yaml</code> on the <code>Control</code> node. Fabric will sustain objects within the original wiring diagram. In order to remove any of the object that is described in this chapter, dedicated API object should be first removed from this file. It's recommended to reapply <code>hh-wiring.yaml</code> after changing it's internals.</p> <ul> <li>If the <code>Switch</code> is a <code>Leaf</code> switch (including <code>Mixed</code> and <code>Border</code> leaf configuration) all <code>VPCAttachments</code> bound to all switches <code>Connections</code> must be removed first. If the <code>Switch</code> was used for <code>ExternalPeering</code> all <code>ExternalAttachment</code> object that are bound to <code>Connections</code> of the <code>Switch</code> must be removed.</li> <li>All connections of the <code>Switch</code> must be removed.</li> <li><code>Switch</code> and <code>Agent</code> object can be removed.</li> </ul>"},{"location":"user-guide/vpcs/","title":"VPCs and Namespaces","text":""},{"location":"user-guide/vpcs/#vpc","title":"VPC","text":"<p>Virtual Private Cloud, similar to the public cloud VPC it provides an isolated private network for the resources with support for multiple subnets each with user-provided VLANs and on-demand DHCP.</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: VPC\nmetadata:\n  name: vpc-1\n  namespace: default\nspec:\n  ipv4Namespace: default # Limits to which subnets could be used by VPC to guarantee non-overlapping IPv4 ranges\n  vlanNamespace: default # Limits to which switches VPC could be attached to guarantee non-overlapping VLANs\n  subnets:\n    default: # Each subnet is named, \"default\" subnet isn't required, but actively used by CLI\n      dhcp:\n        enable: true # On-demand DHCP server\n        range: # Optionally, start/end range could be specified\n          start: 10.10.1.10\n      subnet: 10.10.1.0/24 # User-defined subnet from ipv4 namespace\n      vlan: \"1001\" # User-defined VLAN from vlan namespace\n    thrird-party-dhcp: # Another subnet\n      dhcp:\n        relay: 10.99.0.100/24 # Use third-party DHCP server (DHCP relay configuration), access to it could be enabled using StaticExternal connection\n      subnet: \"10.10.2.0/24\"\n      vlan: \"1002\"\n    another-subnet: # Minimal configuration is just a name, subnet and VLAN\n      subnet: 10.10.100.0/24\n      vlan: \"1100\"\n</code></pre> <p>In case if you're using thirt-party DHCP server by configuring <code>spec.subnets.&lt;subnet&gt;.dhcp.relay</code> additional information will be added to the DHCP packet it forwards to the DHCP server to make it possible to identify the VPC and subnet. The information is added under the RelayAgentInfo option(82) on the DHCP packet. The relay sets two suboptions in the packet</p> <ul> <li>VirtualSubnetSelection -- (suboption 151) is populated with the VRF which uniquely idenitifies a VPC on the Hedgehog   Fabric and will be in <code>VrfV&lt;VPC-name&gt;</code> format, e.g. <code>VrfVvpc-1</code> for VPC named <code>vpc-1</code> in the Fabric API</li> <li>CircuitID -- (suboption 1) identifies the VLAN which together with VRF (VPC) name maps to a specific VPC subnet</li> </ul>"},{"location":"user-guide/vpcs/#vpcattachment","title":"VPCAttachment","text":"<p>Represents a specific VPC subnet assignment to the <code>Connection</code> object which means exact server port to a VPC binding. It basically leads to the VPC being available on the specific server port(s) on a subnet VLAN.</p> <p>VPC could be attached to a switch which is a part of the VLAN namespace used by the VPC.</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: VPCAttachment\nmetadata:\n  name: vpc-1-server-1--mclag--s5248-01--s5248-02\n  namespace: default\nspec:\n  connection: server-1--mclag--s5248-01--s5248-02 # Connection name representing the server port(s)\n  subnet: vpc-1/default # VPC subnet name\n</code></pre>"},{"location":"user-guide/vpcs/#vpcpeering","title":"VPCPeering","text":"<p>It enables VPC to VPC connectivity. There are tw o types of VPC peering:</p> <ul> <li>Local - peering is implemented on the same switches where VPCs are attached</li> <li>Remote - peering is implemented on the border/mixed leafs defined by the <code>SwitchGroup</code> object</li> </ul> <p>VPC peering is only possible between VPCs attached to the same IPv4 namespace.</p> <p>Local:</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: VPCPeering\nmetadata:\n  name: vpc-1--vpc-3\n  namespace: default\nspec:\n  permit: # Defines a pair of VPCs to peer\n  - vpc-1: {} # meaning all subnets of two VPCs will be able to communicate to each other\n    vpc-3: {} # more advanced filtering will be supported in future releases\n</code></pre> <p>Remote:</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: VPCPeering\nmetadata:\n  name: vpc-1--vpc-2\n  namespace: default\nspec:\n  permit:\n  - vpc-1: {}\n    vpc-2: {}\n  remote: border # indicates a switch group to implement the peering on\n</code></pre>"},{"location":"user-guide/vpcs/#ipv4namespace","title":"IPv4Namespace","text":"<p>Defines non-overlapping VLAN ranges for attaching servers. Each switch belongs to a list of VLAN namespaces with non-overlapping VLAN ranges.</p> <pre><code>apiVersion: vpc.githedgehog.com/v1alpha2\nkind: IPv4Namespace\nmetadata:\n  name: default\n  namespace: default\nspec:\n  subnets: # List of the subnets that VPCs can pick their subnets from\n  - 10.10.0.0/16\n</code></pre>"},{"location":"user-guide/vpcs/#vlannamespace","title":"VLANNamespace","text":"<p>Defines non-overlapping IPv4 ranges for VPC subnets. Each VPC belongs to a specific IPv4 namespace.</p> <pre><code>apiVersion: wiring.githedgehog.com/v1alpha2\nkind: VLANNamespace\nmetadata:\n  name: default\n  namespace: default\nspec:\n  ranges: # List of VLAN ranges that VPCs can pick their subnet VLANs from\n  - from: 1000\n    to: 2999\n</code></pre>"},{"location":"vlab/demo/","title":"Demo on VLAB","text":"<p>Goal of this demo is to show how to use VPCs, attach and peer them and test connectivity between the servers. Examples are based on the default VLAB topology.</p> <p>You can find instructions on how to setup VLAB in the Overview and Running VLAB sections.</p>"},{"location":"vlab/demo/#default-topology","title":"Default topology","text":"<p>The default topology is Spine-Leaf with 2 spines, 2 MCLAG leafs and 1 non-MCLAG leaf. Optionally, you can choose to run default Collapsed Core topology using <code>--fabric-mode collapsed-core</code> (or <code>-m collapsed-core</code>) flag which only conisists of 2 switches.</p> <p>For more details on the customizing topologies see Running VLAB section.</p> <p>In the default topology, the following Control Node and Switch VMs are created:</p> <pre><code>graph TD\n    CN[Control Node]\n\n    S1[Spine 1]\n    S2[Spine 2]\n\n    L1[MCLAG Leaf 1]\n    L2[MCLAG Leaf 2]\n    L3[Leaf 3]\n\n    CN --&gt; L1\n    CN --&gt; L2\n\n    S1 --&gt; L1\n    S1 --&gt; L2\n    S2 --&gt; L2\n    S2 --&gt; L3</code></pre> <p>As well as test servers:</p> <pre><code>graph TD\n    L1[MCLAG Leaf 1]\n    L2[MCLAG Leaf 2]\n    L3[Leaf 3]\n\n    TS1[Test Server 1]\n    TS2[Test Server 2]\n    TS3[Test Server 3]\n    TS4[Test Server 4]\n    TS5[Test Server 5]\n    TS6[Test Server 6]\n\n    TS1 --&gt; L1\n    TS1 --&gt; L2\n\n    TS2 --&gt; L1\n    TS2 --&gt; L2\n\n    TS3 --&gt; L1\n    TS4 --&gt; L2\n\n    TS5 --&gt; L3\n    TS6 --&gt; L4</code></pre>"},{"location":"vlab/demo/#creating-and-attaching-vpcs","title":"Creating and attaching VPCs","text":"<p>You can create and attach VPCs to the VMs using the <code>kubectl fabric vpc</code> command on control node or outside of cluster using the kubeconfig. For example, run following commands to create a 2 VPCs with a single subnet each, DHCP server enabled with optional IP address range start defined and attach them to some test servers:</p> <pre><code>core@control-1 ~ $ kubectl get conn | grep server\nserver-01--mclag--leaf-01--leaf-02   mclag          5h13m\nserver-02--mclag--leaf-01--leaf-02   mclag          5h13m\nserver-03--unbundled--leaf-01        unbundled      5h13m\nserver-04--bundled--leaf-02          bundled        5h13m\nserver-05--unbundled--leaf-03        unbundled      5h13m\nserver-06--bundled--leaf-03          bundled        5h13m\n\ncore@control-1 ~ $ kubectl fabric vpc create --name vpc-1 --subnet 10.0.1.0/24 --vlan 1001 --dhcp --dhcp-start 10.0.1.10\n06:48:46 INF VPC created name=vpc-1\n\ncore@control-1 ~ $ kubectl fabric vpc create --name vpc-2 --subnet 10.0.2.0/24 --vlan 1002 --dhcp --dhcp-start 10.0.2.10\n06:49:04 INF VPC created name=vpc-2\n\ncore@control-1 ~ $ kubectl fabric vpc attach --vpc-subnet vpc-1/default --connection server-01--mclag--leaf-01--leaf-02\n06:49:24 INF VPCAttachment created name=vpc-1--default--server-01--mclag--leaf-01--leaf-02\n\ncore@control-1 ~ $ kubectl fabric vpc attach --vpc-subnet vpc-2/default --connection server-02--mclag--leaf-01--leaf-02\n06:49:34 INF VPCAttachment created name=vpc-2--default--server-02--mclag--leaf-01--leaf-02\n</code></pre> <p>VPC subnet should belong to some IPv4Namespace, default one in the VLAB is <code>10.0.0.0/16</code>:</p> <pre><code>core@control-1 ~ $ kubectl get ipns\nNAME      SUBNETS           AGE\ndefault   [\"10.0.0.0/16\"]   5h14m\n</code></pre> <p>After you created VPCs and VPCAttachments, you can check the status of the agents to make sure that requested configuration was apploed to the switches:</p> <pre><code>core@control-1 ~ $ kubectl get agents\nNAME       ROLE          DESCR           APPLIED   APPLIEDG   CURRENTG   VERSION\nleaf-01    server-leaf   VS-01 MCLAG 1   2m2s      5          5          v0.23.0\nleaf-02    server-leaf   VS-02 MCLAG 1   2m2s      4          4          v0.23.0\nleaf-03    server-leaf   VS-03           112s      5          5          v0.23.0\nspine-01   spine         VS-04           16m       3          3          v0.23.0\nspine-02   spine         VS-05           18m       4          4          v0.23.0\n</code></pre> <p>As you can see columns <code>APPLIED</code> and <code>APPLIEDG</code> are equal which means that requested configuration was applied.</p>"},{"location":"vlab/demo/#setting-up-networking-on-test-servers","title":"Setting up networking on test servers","text":"<p>You can use <code>hhfab vlab ssh</code> on the host to ssh into the test servers and configure networking there. For example, for both server-01 (MCLAG attached to both leaf-01 and leaf-02) we need to configure bond with a vlan on top of it and for the server-05 (single-homed unbundled attached to leaf-03) we need to configure just a vlan and they both will get an IP address from the DHCP server. You can use <code>ip</code> command to configure networking on the servers or use little helper preinstalled by Fabricator on test servers.</p> <p>For server-01:</p> <pre><code>core@server-01 ~ $ hhnet cleanup\ncore@server-01 ~ $ hhnet bond 1001 enp2s1 enp2s2\n10.0.1.10/24\ncore@server-01 ~ $ ip a\n...\n3: enp2s1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000\n    link/ether 06:5a:e8:38:3b:ea brd ff:ff:ff:ff:ff:ff permaddr 0c:20:12:fe:01:01\n4: enp2s2: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000\n    link/ether 06:5a:e8:38:3b:ea brd ff:ff:ff:ff:ff:ff permaddr 0c:20:12:fe:01:02\n6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 06:5a:e8:38:3b:ea brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::45a:e8ff:fe38:3bea/64 scope link\n       valid_lft forever preferred_lft forever\n7: bond0.1001@bond0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 06:5a:e8:38:3b:ea brd ff:ff:ff:ff:ff:ff\n    inet 10.0.1.10/24 metric 1024 brd 10.0.1.255 scope global dynamic bond0.1001\n       valid_lft 86396sec preferred_lft 86396sec\n    inet6 fe80::45a:e8ff:fe38:3bea/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre> <p>And for server-02:</p> <pre><code>core@server-02 ~ $ hhnet cleanup\ncore@server-02 ~ $ hhnet bond 1002 enp2s1 enp2s2\n10.0.2.10/24\ncore@server-02 ~ $ ip a\n...\n3: enp2s1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000\n    link/ether 5e:10:b1:f7:d0:4c brd ff:ff:ff:ff:ff:ff permaddr 0c:20:12:fe:02:01\n4: enp2s2: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000\n    link/ether 5e:10:b1:f7:d0:4c brd ff:ff:ff:ff:ff:ff permaddr 0c:20:12:fe:02:02\n8: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 5e:10:b1:f7:d0:4c brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::5c10:b1ff:fef7:d04c/64 scope link\n       valid_lft forever preferred_lft forever\n9: bond0.1002@bond0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 5e:10:b1:f7:d0:4c brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.10/24 metric 1024 brd 10.0.2.255 scope global dynamic bond0.1002\n       valid_lft 86185sec preferred_lft 86185sec\n    inet6 fe80::5c10:b1ff:fef7:d04c/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"vlab/demo/#testing-connectivity-before-peering","title":"Testing connectivity before peering","text":"<p>You can test connectivity between the servers before peering the switches using <code>ping</code> command:</p> <pre><code>core@server-01 ~ $ ping 10.0.2.10\nPING 10.0.2.10 (10.0.2.10) 56(84) bytes of data.\nFrom 10.0.1.1 icmp_seq=1 Destination Net Unreachable\nFrom 10.0.1.1 icmp_seq=2 Destination Net Unreachable\nFrom 10.0.1.1 icmp_seq=3 Destination Net Unreachable\n^C\n--- 10.0.2.10 ping statistics ---\n3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2003ms\n</code></pre> <pre><code>core@server-02 ~ $ ping 10.0.1.10\nPING 10.0.1.10 (10.0.1.10) 56(84) bytes of data.\nFrom 10.0.2.1 icmp_seq=1 Destination Net Unreachable\nFrom 10.0.2.1 icmp_seq=2 Destination Net Unreachable\nFrom 10.0.2.1 icmp_seq=3 Destination Net Unreachable\n^C\n--- 10.0.1.10 ping statistics ---\n3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2004ms\n</code></pre>"},{"location":"vlab/demo/#peering-vpcs-and-testing-connectivity","title":"Peering VPCs and testing connectivity","text":"<p>To enable connectivity between the VPCs, you need to peer them using <code>kubectl fabric vpc peer</code> command:</p> <pre><code>core@control-1 ~ $ kubectl fabric vpc peer --vpc vpc-1 --vpc vpc-2\n07:04:58 INF VPCPeering created name=vpc-1--vpc-2\n</code></pre> <p>Make sure to wait until the peering is applied to the switches using <code>kubectl get agents</code> command. After that you can test connectivity between the servers again:</p> <pre><code>core@server-01 ~ $ ping 10.0.2.10\nPING 10.0.2.10 (10.0.2.10) 56(84) bytes of data.\n64 bytes from 10.0.2.10: icmp_seq=1 ttl=62 time=6.25 ms\n64 bytes from 10.0.2.10: icmp_seq=2 ttl=62 time=7.60 ms\n64 bytes from 10.0.2.10: icmp_seq=3 ttl=62 time=8.60 ms\n^C\n--- 10.0.2.10 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2004ms\nrtt min/avg/max/mdev = 6.245/7.481/8.601/0.965 ms\n</code></pre> <pre><code>core@server-02 ~ $ ping 10.0.1.10\nPING 10.0.1.10 (10.0.1.10) 56(84) bytes of data.\n64 bytes from 10.0.1.10: icmp_seq=1 ttl=62 time=5.44 ms\n64 bytes from 10.0.1.10: icmp_seq=2 ttl=62 time=6.66 ms\n64 bytes from 10.0.1.10: icmp_seq=3 ttl=62 time=4.49 ms\n^C\n--- 10.0.1.10 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2004ms\nrtt min/avg/max/mdev = 4.489/5.529/6.656/0.886 ms\n</code></pre> <p>If you will delete VPC peering using command following command and wait for the agent to apply configuration on the switches, you will see that connectivity will be lost again:</p> <pre><code>core@control-1 ~ $ kubectl delete vpcpeering/vpc-1--vpc-2\nvpcpeering.vpc.githedgehog.com \"vpc-1--vpc-2\" deleted\n</code></pre> <pre><code>core@server-01 ~ $ ping 10.0.2.10\nPING 10.0.2.10 (10.0.2.10) 56(84) bytes of data.\nFrom 10.0.1.1 icmp_seq=1 Destination Net Unreachable\nFrom 10.0.1.1 icmp_seq=2 Destination Net Unreachable\nFrom 10.0.1.1 icmp_seq=3 Destination Net Unreachable\n^C\n--- 10.0.2.10 ping statistics ---\n3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2004ms\n</code></pre> <p>You can see duplicate packets in the output of the <code>ping</code> command between some of the servers. This is expected behavior and is caused by the limitations in the VLAB environment.</p> <pre><code>core@server-01 ~ $ ping 10.0.5.10\nPING 10.0.5.10 (10.0.5.10) 56(84) bytes of data.\n64 bytes from 10.0.5.10: icmp_seq=1 ttl=62 time=9.58 ms\n64 bytes from 10.0.5.10: icmp_seq=1 ttl=62 time=9.58 ms (DUP!)\n64 bytes from 10.0.5.10: icmp_seq=2 ttl=62 time=6.99 ms\n64 bytes from 10.0.5.10: icmp_seq=2 ttl=62 time=6.99 ms (DUP!)\n64 bytes from 10.0.5.10: icmp_seq=3 ttl=62 time=9.59 ms\n64 bytes from 10.0.5.10: icmp_seq=3 ttl=62 time=9.60 ms (DUP!)\n^C\n--- 10.0.5.10 ping statistics ---\n3 packets transmitted, 3 received, +3 duplicates, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 6.987/8.720/9.595/1.226 ms\n</code></pre>"},{"location":"vlab/demo/#using-vpcs-with-overlapping-subnets","title":"Using VPCs with overlapping subnets","text":"<p>First of all, we'll need to make sure that we have a second IPv4Namespace with the same subnet as default one:</p> <pre><code>core@control-1 ~ $ kubectl get ipns\nNAME      SUBNETS           AGE\ndefault   [\"10.0.0.0/16\"]   24m\n\ncore@control-1 ~ $ cat &lt;&lt;EOF &gt; ipns-2.yaml\napiVersion: vpc.githedgehog.com/v1alpha2\nkind: IPv4Namespace\nmetadata:\n  name: ipns-2\n  namespace: default\nspec:\n  subnets:\n  - 10.0.0.0/16\nEOF\n\ncore@control-1 ~ $ kubectl apply -f ipns-2.yaml\nipv4namespace.vpc.githedgehog.com/ipns-2 created\n\ncore@control-1 ~ $ kubectl get ipns\nNAME      SUBNETS           AGE\ndefault   [\"10.0.0.0/16\"]   30m\nipns-2    [\"10.0.0.0/16\"]   8s\n</code></pre> <p>Let's assume that <code>vpc-1</code> already exists and is attached to <code>server-01</code> (see Creating and attaching VPCs). Now we can create <code>vpc-3</code> with the same subnet as <code>vpc-1</code> (but in the different IPv4Namespace) and attach it to the <code>server-03</code>:</p> <pre><code>core@control-1 ~ $ cat &lt;&lt;EOF &gt; vpc-3.yaml\napiVersion: vpc.githedgehog.com/v1alpha2\nkind: VPC\nmetadata:\n  name: vpc-1\n  namespace: default\nspec:\n  ipv4Namespace: ipns-2\n  subnets:\n    default:\n      dhcp:\n        enable: true\n        range:\n          start: 10.0.1.10\n      subnet: 10.0.1.0/24\n      vlan: \"2001\"\n  vlanNamespace: default\nEOF\n\ncore@control-1 ~ $ kubectl apply -f vpc-3.yaml\n</code></pre> <p>At that point you can setup networking on the <code>server-03</code> same as for <code>server-01</code> and <code>server-02</code> in a previous sections and see that we have now <code>server-01</code> and <code>server-03</code> with the IP addresses from the same subnets.</p>"},{"location":"vlab/overview/","title":"Overview","text":"<p>It's possible to run Hedgehog Fabric in a fully virtual environment using QEMU/KVM and SONiC Virtual Switch (VS). It's a great way to try out Fabric and learn about its looka and feel, API, capabilities and etc. It's not suitable for any data plane or performance testing as well as not for production use.</p> <p>In the VLAB all switches will start as an empty VMs with only ONiE image on them and will go through the whole discovery, boot and installation process like on the real hardware.</p>"},{"location":"vlab/overview/#overview_1","title":"Overview","text":"<p>The <code>hhfab</code> CLI provides a special command <code>vlab</code> to manage the virtual labs. It allows to run set of virtual machines to simulate the Fabric infrastructure including control node, switches, test servers and automatically runs installer to get Fabric up and running.</p> <p>You can find more information about getting <code>hhfab</code> in the download section.</p>"},{"location":"vlab/overview/#system-requirements","title":"System Requirements","text":"<p>Currently, it's only tested on Ubuntu 22.04 LTS, but should work on any Linux distribution with QEMU/KVM support and fairly up-to-date packages.</p> <p>Following packages needs to be installed: <code>qemu-kvm swtpm-tools tpm2-tools socat</code> and docker will be required to login into OCI registry.</p> <p>By default, VLAB topology is Spine-Leaf with 2 spines, 2 MCLAG leafs and 1 non-MCLAG leaf. Optionally, you can choose to run default Collapsed Core topology using <code>--fabric-mode collapsed-core</code> (or <code>-m collapsed-core</code>) flag which only conisists of 2 switches.</p> <p>You can calculate the system requirements based on the allocated resources to the VMs using the following table:</p> Device vCPU RAM Disk Control Node 6 6GB 100GB Test Server 2 768MB 10GB Switch 4 5GB 50GB <p>Which gives approximately the following requirements for the default topologies:</p> <ul> <li>Spine-Leaf: 38 vCPUs, 36352 MB, 410 GB disk</li> <li>Collapsed Core: 22 vCPUs, 19456 MB, 240 GB disk</li> </ul> <p>Usually, non of the VMs will reach 100% utilization of the allocated resources, but as a rule of thumb you should make sure that you have at least allocated RAM and disk space for all VMs.</p> <p>NVMe SSD for VM disks is highly recommended.</p>"},{"location":"vlab/overview/#installing-prerequisites","title":"Installing prerequisites","text":"<p>On Ubuntu 22.04 LTS you can install all required packages using the following commands:</p> <pre><code>curl -fsSL https://get.docker.com -o install-docker.sh\nsudo sh install-docker.sh\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> <pre><code>sudo apt install -y qemu-kvm swtpm-tools tpm2-tools socat\nsudo usermod -aG kvm $USER\nnewgrp kvm\nkvm-ok\n</code></pre> <p>Good output of the <code>kvm-ok</code> command should look like this:</p> <pre><code>ubuntu@docs:~$ kvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre>"},{"location":"vlab/overview/#next-steps","title":"Next steps","text":"<ul> <li>Running VLAB</li> </ul>"},{"location":"vlab/running/","title":"Running VLAB","text":"<p>Please, make sure to follow prerequisites and check system requirements in the VLAB Overview section before running VLAB.</p>"},{"location":"vlab/running/#initialize-vlab","title":"Initialize VLAB","text":"<p>As a first step you need to initialize Fabricator for the VLAB by running <code>hhfab init --preset vlab</code> (or <code>-p vlab</code>). It supports a lot of customization options which you can find by adding <code>--help</code> to the command. If you want to tune the topology used for the VLAB you can use <code>--fabric-mode</code> (or <code>-m</code>) flag to choose between <code>spine-leaf</code> (default) and <code>collapsed-core</code> topologies as well as you can configure the number of spines, leafs, connections and etc. For example, <code>--spines-count</code> and <code>--mclag-leafs-count</code> flags allows to set number of spines and MCLAG leafs respectively.</p> <p>So, by default you'll get 2 spines, 2 MCLAG leafs and 1 non-MCLAG leaf with 2 fabric connections (between each spine and leaf), 2 MCLAG peer links and 2 MCLAG session links as well as 2 loopbacks per leaf for implementing VPC Loopback workaround.</p> <pre><code>ubuntu@docs:~$ hhfab init -p vlab\n01:17:44 INF Generating wiring from gen flags\n01:17:44 INF Building wiring diagram fabricMode=spine-leaf chainControlLink=false controlLinksCount=0\n01:17:44 INF                     &gt;&gt;&gt; spinesCount=2 fabricLinksCount=2\n01:17:44 INF                     &gt;&gt;&gt; mclagLeafsCount=2 orphanLeafsCount=1\n01:17:44 INF                     &gt;&gt;&gt; mclagSessionLinks=2 mclagPeerLinks=2\n01:17:44 INF                     &gt;&gt;&gt; vpcLoopbacks=2\n01:17:44 WRN Wiring is not hydrated, hydrating reason=\"error validating wiring: ASN not set for switch leaf-01\"\n01:17:44 INF Initialized preset=vlab fabricMode=spine-leaf config=.hhfab/config.yaml wiring=.hhfab/wiring.yaml\n</code></pre> <p>Or if you want to run Collapsed Core topology with 2 MCLAG switches:</p> <pre><code>ubuntu@docs:~$ hhfab init -p vlab -m collapsed-core\n01:20:07 INF Generating wiring from gen flags\n01:20:07 INF Building wiring diagram fabricMode=collapsed-core chainControlLink=false controlLinksCount=0\n01:20:07 INF                     &gt;&gt;&gt; mclagLeafsCount=2 orphanLeafsCount=0\n01:20:07 INF                     &gt;&gt;&gt; mclagSessionLinks=2 mclagPeerLinks=2\n01:20:07 INF                     &gt;&gt;&gt; vpcLoopbacks=2\n01:20:07 WRN Wiring is not hydrated, hydrating reason=\"error validating wiring: ASN not set for switch leaf-01\"\n01:20:07 INF Initialized preset=vlab fabricMode=collapsed-core config=.hhfab/config.yaml wiring=.hhfab/wiring.yaml\n</code></pre> <p>Or you can run custom topology with 2 spines, 4 MCLAG leafs and 2 non-MCLAG leafs using flags:</p> <pre><code>ubuntu@docs:~$ hhfab init -p vlab --mclag-leafs-count 4 --orphan-leafs-count 2\n01:21:53 INF Generating wiring from gen flags\n01:21:53 INF Building wiring diagram fabricMode=spine-leaf chainControlLink=false controlLinksCount=0\n01:21:53 INF                     &gt;&gt;&gt; spinesCount=2 fabricLinksCount=2\n01:21:53 INF                     &gt;&gt;&gt; mclagLeafsCount=4 orphanLeafsCount=2\n01:21:53 INF                     &gt;&gt;&gt; mclagSessionLinks=2 mclagPeerLinks=2\n01:21:53 INF                     &gt;&gt;&gt; vpcLoopbacks=2\n01:21:53 WRN Wiring is not hydrated, hydrating reason=\"error validating wiring: ASN not set for switch leaf-01\"\n01:21:53 INF Initialized preset=vlab fabricMode=spine-leaf config=.hhfab/config.yaml wiring=.hhfab/wiring.yaml\n</code></pre> <p>Additionally, you can do extra Fabric configuration using flags on <code>init</code> command or by passing config file, more information about it is available in the Fabric Configuration section.</p> <p>Once you have initialized the VLAB you need to download all artifacts and build the installer using <code>hhfab build</code> command. It will automatically download all required artifacts from the OCI registry and build the installer as well as all other prerequisites for running the VLAB.</p>"},{"location":"vlab/running/#build-the-installer-and-vlab","title":"Build the installer and VLAB","text":"<pre><code>ubuntu@docs:~$ hhfab build\n01:23:33 INF Building component=base\n01:23:33 WRN Attention! Development mode enabled - this is not secure! Default users and keys will be created.\n...\n01:23:33 INF Building component=control-os\n01:23:33 INF Building component=k3s\n01:23:33 INF Downloading name=m.l.hhdev.io:31000/githedgehog/k3s:v1.27.4-k3s1 to=.hhfab/control-install\nCopying k3s-airgap-images-amd64.tar.gz  187.36 MiB / 187.36 MiB   \u2819   0.00 b/s done\nCopying k3s                               56.50 MiB / 56.50 MiB   \u2819   0.00 b/s done\n01:23:35 INF Building component=zot\n01:23:35 INF Downloading name=m.l.hhdev.io:31000/githedgehog/zot:v1.4.3 to=.hhfab/control-install\nCopying zot-airgap-images-amd64.tar.gz  19.24 MiB / 19.24 MiB   \u2838   0.00 b/s done\n01:23:35 INF Building component=misc\n01:23:35 INF Downloading name=m.l.hhdev.io:31000/githedgehog/fabricator/k9s:v0.27.4 to=.hhfab/control-install\nCopying k9s  57.75 MiB / 57.75 MiB   \u283c   0.00 b/s done\n...\n01:25:40 INF Planned bundle=control-install name=fabric-api-chart op=\"push fabric/charts/fabric-api:v0.23.0\"\n01:25:40 INF Planned bundle=control-install name=fabric-image op=\"push fabric/fabric:v0.23.0\"\n01:25:40 INF Planned bundle=control-install name=fabric-chart op=\"push fabric/charts/fabric:v0.23.0\"\n01:25:40 INF Planned bundle=control-install name=fabric-agent-seeder op=\"push fabric/agent/x86_64:latest\"\n01:25:40 INF Planned bundle=control-install name=fabric-agent op=\"push fabric/agent:v0.23.0\"\n...\n01:25:40 INF Recipe created bundle=control-install actions=67\n01:25:40 INF Creating recipe bundle=server-install\n01:25:40 INF Planned bundle=server-install name=toolbox op=\"file /opt/hedgehog/toolbox.tar\"\n01:25:40 INF Planned bundle=server-install name=toolbox-load op=\"exec ctr\"\n01:25:40 INF Planned bundle=server-install name=hhnet op=\"file /opt/bin/hhnet\"\n01:25:40 INF Recipe created bundle=server-install actions=3\n01:25:40 INF Building done took=2m6.813384532s\n01:25:40 INF Packing bundle=control-install target=control-install.tgz\n01:25:45 INF Packing bundle=server-install target=server-install.tgz\n01:25:45 INF Packing done took=5.67007384s\n</code></pre> <p>As soon as it's done you can run the VLAB using <code>hhfab vlab up</code> command. It will automatically start all VMs and run the installers on the control node and test servers. It will take some time for all VMs to come up and for the installer to finish, you will see the progress in the output. If you stop the command, it'll stop all VMs, and you can re-run it to get VMs back up and running.</p>"},{"location":"vlab/running/#run-vms-and-installers","title":"Run VMs and installers","text":"<pre><code>ubuntu@docs:~$ hhfab vlab up\n01:29:13 INF Starting VLAB server... basedir=.hhfab/vlab-vms vm-size=\"\" dry-run=false\n01:29:13 INF VM id=0 name=control-1 type=control\n01:29:13 INF VM id=1 name=server-01 type=server\n01:29:13 INF VM id=2 name=server-02 type=server\n01:29:13 INF VM id=3 name=server-03 type=server\n01:29:13 INF VM id=4 name=server-04 type=server\n01:29:13 INF VM id=5 name=server-05 type=server\n01:29:13 INF VM id=6 name=server-06 type=server\n01:29:13 INF VM id=7 name=leaf-01 type=switch-vs\n01:29:13 INF VM id=8 name=leaf-02 type=switch-vs\n01:29:13 INF VM id=9 name=leaf-03 type=switch-vs\n01:29:13 INF VM id=10 name=spine-01 type=switch-vs\n01:29:13 INF VM id=11 name=spine-02 type=switch-vs\n01:29:13 INF Total VM resources cpu=\"38 vCPUs\" ram=\"36352 MB\" disk=\"410 GB\"\n...\n01:29:13 INF Preparing VM id=0 name=control-1 type=control\n01:29:13 INF Copying files  from=.hhfab/control-os/ignition.json to=.hhfab/vlab-vms/control-1/ignition.json\n01:29:13 INF Copying files  from=.hhfab/vlab-files/flatcar.img to=.hhfab/vlab-vms/control-1/os.img\n 947.56 MiB / 947.56 MiB [==========================================================] 5.13 GiB/s done\n01:29:14 INF Copying files  from=.hhfab/vlab-files/flatcar_efi_code.fd to=.hhfab/vlab-vms/control-1/efi_code.fd\n01:29:14 INF Copying files  from=.hhfab/vlab-files/flatcar_efi_vars.fd to=.hhfab/vlab-vms/control-1/efi_vars.fd\n01:29:14 INF Resizing VM image (may require sudo password) name=control-1\n01:29:17 INF Initializing TPM name=control-1\n...\n01:29:46 INF Installing VM name=control-1 type=control\n01:29:46 INF Installing VM name=server-01 type=server\n01:29:46 INF Installing VM name=server-02 type=server\n01:29:46 INF Installing VM name=server-03 type=server\n01:29:47 INF Installing VM name=server-04 type=server\n01:29:47 INF Installing VM name=server-05 type=server\n01:29:47 INF Installing VM name=server-06 type=server\n01:29:49 INF Running VM id=0 name=control-1 type=control\n01:29:49 INF Running VM id=1 name=server-01 type=server\n01:29:49 INF Running VM id=2 name=server-02 type=server\n01:29:49 INF Running VM id=3 name=server-03 type=server\n01:29:50 INF Running VM id=4 name=server-04 type=server\n01:29:50 INF Running VM id=5 name=server-05 type=server\n01:29:50 INF Running VM id=6 name=server-06 type=server\n01:29:50 INF Running VM id=7 name=leaf-01 type=switch-vs\n01:29:50 INF Running VM id=8 name=leaf-02 type=switch-vs\n01:29:51 INF Running VM id=9 name=leaf-03 type=switch-vs\n01:29:51 INF Running VM id=10 name=spine-01 type=switch-vs\n01:29:51 INF Running VM id=11 name=spine-02 type=switch-vs\n...\n01:30:41 INF VM installed name=server-06 type=server installer=server-install\n01:30:41 INF VM installed name=server-01 type=server installer=server-install\n01:30:41 INF VM installed name=server-02 type=server installer=server-install\n01:30:41 INF VM installed name=server-04 type=server installer=server-install\n01:30:41 INF VM installed name=server-03 type=server installer=server-install\n01:30:41 INF VM installed name=server-05 type=server installer=server-install\n...\n01:31:04 INF Running installer on VM name=control-1 type=control installer=control-install\n...\n01:35:15 INF Done took=3m39.586394608s\n01:35:15 INF VM installed name=control-1 type=control installer=control-install\n</code></pre> <p>After you see <code>VM installed name=control-1</code>, it means that the installer has finished and you can get into the control node and other VMs to watch the Fabric coming up and switches getting provisioned.</p>"},{"location":"vlab/running/#configuring-vlab-vms","title":"Configuring VLAB VMs","text":"<p>By default, all test server VMs are isolated and have no connectivity to the host or internet. You can configure it using <code>hhfab vlab up --restrict-servers=false</code> flag to allow the test servers to access the internet and the host. It will mean that VMs will have default route pointing to the host which means in case of the VPC peering you'll need to configure test server VMs to use the VPC attachment as a default route (or just some specific subnets).</p> <p>Additionally, you can configure the size of all VMs using <code>hhfab vlab up --vm-size &lt;size&gt;</code> flag. It will allow you to choose from one of the presets (compact, default, full and huge) to get the control, switch and server VMs of different sizes.</p>"},{"location":"vlab/running/#default-credentials","title":"Default credentials","text":"<p>Fabricator will create default users and keys for you to login into the control node and test servers as well as for the SONiC Virtual Switches.</p> <p>Default user with passwordless sudo for the control node and test servers is <code>core</code> with password <code>HHFab.Admin!</code>. Admin user with full access and passwordless sudo for the switches is <code>admin</code> with password <code>HHFab.Admin!</code>. Read-only, non-sudo user with access only to the switch CLI for the switches is <code>op</code> with password <code>HHFab.Op!</code>.</p>"},{"location":"vlab/running/#accessing-the-vlab","title":"Accessing the VLAB","text":"<p>The <code>hhfab vlab</code> command provides <code>ssh</code> and <code>serial</code> subcommands to access the VMs. You can use <code>ssh</code> to get into the control node and test servers after the VMs are started. You can use <code>serial</code> to get into the switch VMs while they are provisioning and installing the software. After switches are installed you can use <code>ssh</code> to get into them.</p> <p>You can select device you want to access or pass the name using the <code>--vm</code> flag.</p> <pre><code>ubuntu@docs:~$ hhfab vlab ssh\nUse the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190  and / toggles search\nSSH to VM:\n  \ud83e\udd94 control-1\n  server-01\n  server-02\n  server-03\n  server-04\n  server-05\n  server-06\n  leaf-01\n  leaf-02\n  leaf-03\n  spine-01\n  spine-02\n\n----------- VM Details ------------\nID:             0\nName:           control-1\nReady:          true\nBasedir:        .hhfab/vlab-vms/control-1\n</code></pre> <p>On the control node you'll have access to the kubectl, Fabric CLI and k9s to manage the Fabric. You can find information about the switches provisioning by running <code>kubectl get agents -o wide</code>. It usually takes about 10-15 minutes for the switches to get installed.</p> <p>After switches are provisioned you will see something like this:</p> <pre><code>core@control-1 ~ $ kubectl get agents -o wide\nNAME       ROLE          DESCR           HWSKU                      ASIC   HEARTBEAT   APPLIED   APPLIEDG   CURRENTG   VERSION   SOFTWARE                ATTEMPT   ATTEMPTG   AGE\nleaf-01    server-leaf   VS-01 MCLAG 1   DellEMC-S5248f-P-25G-DPB   vs     30s         5m5s      4          4          v0.23.0   4.1.1-Enterprise_Base   5m5s      4          10m\nleaf-02    server-leaf   VS-02 MCLAG 1   DellEMC-S5248f-P-25G-DPB   vs     27s         3m30s     3          3          v0.23.0   4.1.1-Enterprise_Base   3m30s     3          10m\nleaf-03    server-leaf   VS-03           DellEMC-S5248f-P-25G-DPB   vs     18s         3m52s     4          4          v0.23.0   4.1.1-Enterprise_Base   3m52s     4          10m\nspine-01   spine         VS-04           DellEMC-S5248f-P-25G-DPB   vs     26s         3m59s     3          3          v0.23.0   4.1.1-Enterprise_Base   3m59s     3          10m\nspine-02   spine         VS-05           DellEMC-S5248f-P-25G-DPB   vs     19s         3m53s     4          4          v0.23.0   4.1.1-Enterprise_Base   3m53s     4          10m\n</code></pre> <p><code>Heartbeat</code> column shows how long ago the switch has sent the heartbeat to the control node. <code>Applied</code> column shows how long ago the switch has applied the configuration. <code>AppliedG</code> shows the generation of the configuration applied. <code>CurrentG</code> shows the generation of the configuration the switch is supposed to run. If <code>AppliedG</code> and <code>CurrentG</code> are different it means that the switch is in the process of applying the configuration.</p> <p>At that point Fabric is ready and you can use <code>kubectl</code> and <code>kubectl fabric</code> to manage the Fabric. You can find more about it in the Running Demo and User Guide sections.</p>"},{"location":"vlab/running/#getting-main-fabric-objects","title":"Getting main Fabric objects","text":"<p>You can get the main Fabric objects using <code>kubectl get</code> command on the control node. You can find more details about using the Fabric in the User Guide, Fabric API and Fabric CLI sections.</p> <p>For example, to get the list of switches you can run:</p> <pre><code>core@control-1 ~ $ kubectl get switch\nNAME       ROLE          DESCR           GROUPS   LOCATIONUUID                           AGE\nleaf-01    server-leaf   VS-01 MCLAG 1            5e2ae08a-8ba9-599a-ae0f-58c17cbbac67   6h10m\nleaf-02    server-leaf   VS-02 MCLAG 1            5a310b84-153e-5e1c-ae99-dff9bf1bfc91   6h10m\nleaf-03    server-leaf   VS-03                    5f5f4ad5-c300-5ae3-9e47-f7898a087969   6h10m\nspine-01   spine         VS-04                    3e2c4992-a2e4-594b-bbd1-f8b2fd9c13da   6h10m\nspine-02   spine         VS-05                    96fbd4eb-53b5-5a4c-8d6a-bbc27d883030   6h10m\n</code></pre> <p>Similar for the servers:</p> <pre><code>core@control-1 ~ $ kubectl get server\nNAME        TYPE      DESCR                        AGE\ncontrol-1   control   Control node                 6h10m\nserver-01             S-01 MCLAG leaf-01 leaf-02   6h10m\nserver-02             S-02 MCLAG leaf-01 leaf-02   6h10m\nserver-03             S-03 Unbundled leaf-01       6h10m\nserver-04             S-04 Bundled leaf-02         6h10m\nserver-05             S-05 Unbundled leaf-03       6h10m\nserver-06             S-06 Bundled leaf-03         6h10m\n</code></pre> <p>For connections:</p> <pre><code>core@control-1 ~ $ kubectl get connection\nNAME                                 TYPE           AGE\ncontrol-1--mgmt--leaf-01             management     6h11m\ncontrol-1--mgmt--leaf-02             management     6h11m\ncontrol-1--mgmt--leaf-03             management     6h11m\ncontrol-1--mgmt--spine-01            management     6h11m\ncontrol-1--mgmt--spine-02            management     6h11m\nleaf-01--mclag-domain--leaf-02       mclag-domain   6h11m\nleaf-01--vpc-loopback                vpc-loopback   6h11m\nleaf-02--vpc-loopback                vpc-loopback   6h11m\nleaf-03--vpc-loopback                vpc-loopback   6h11m\nserver-01--mclag--leaf-01--leaf-02   mclag          6h11m\nserver-02--mclag--leaf-01--leaf-02   mclag          6h11m\nserver-03--unbundled--leaf-01        unbundled      6h11m\nserver-04--bundled--leaf-02          bundled        6h11m\nserver-05--unbundled--leaf-03        unbundled      6h11m\nserver-06--bundled--leaf-03          bundled        6h11m\nspine-01--fabric--leaf-01            fabric         6h11m\nspine-01--fabric--leaf-02            fabric         6h11m\nspine-01--fabric--leaf-03            fabric         6h11m\nspine-02--fabric--leaf-01            fabric         6h11m\nspine-02--fabric--leaf-02            fabric         6h11m\nspine-02--fabric--leaf-03            fabric         6h11m\n</code></pre> <p>For IPv4 and VLAN namespaces:</p> <pre><code>core@control-1 ~ $ kubectl get ipns\nNAME      SUBNETS           AGE\ndefault   [\"10.0.0.0/16\"]   6h12m\n\ncore@control-1 ~ $ kubectl get vlanns\nNAME      AGE\ndefault   6h12m\n</code></pre>"},{"location":"vlab/running/#reset-vlab","title":"Reset VLAB","text":"<p>To reset VLAB and start over just remove the <code>.hhfab</code> directory and run <code>hhfab init</code> again.</p>"},{"location":"vlab/running/#next-steps","title":"Next steps","text":"<ul> <li>Running Demo</li> </ul>"}]}